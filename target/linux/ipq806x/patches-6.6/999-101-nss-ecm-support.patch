--- a/drivers/net/bonding/bond_main.c
+++ b/drivers/net/bonding/bond_main.c
@@ -3827,11 +3827,15 @@ static bool bond_flow_dissect_without_sk
 	dst = (uint32_t *)pdst;
 
 	if (protocol == htons(ETH_P_IP)) {
+		/* V4 addresses and address type*/
 		fk->addrs.v4addrs.src = src[0];
 		fk->addrs.v4addrs.dst = dst[0];
+		fk->control.addr_type = FLOW_DISSECTOR_KEY_IPV4_ADDRS;
 	} else if (protocol == htons(ETH_P_IPV6)) {
+		/* V6 addresses and address type*/
 		memcpy(&fk->addrs.v6addrs.src, src, sizeof(struct in6_addr));
 		memcpy(&fk->addrs.v6addrs.dst, dst, sizeof(struct in6_addr));
+		fk->control.addr_type = FLOW_DISSECTOR_KEY_IPV6_ADDRS;
 	} else {
 		return false;
 	}
--- a/drivers/net/macvlan.c
+++ b/drivers/net/macvlan.c
@@ -930,6 +930,32 @@ static void macvlan_uninit(struct net_de
 		macvlan_port_destroy(port->dev);
 }
 
+/* Update macvlan statistics processed by offload engines */
+static void macvlan_dev_update_stats(struct net_device *dev,
+				     struct rtnl_link_stats64 *offl_stats,
+				     bool update_mcast_rx_stats)
+{
+	struct vlan_pcpu_stats *stats;
+	struct macvlan_dev *macvlan;
+
+	/* Is this a macvlan? */
+	if (!netif_is_macvlan(dev))
+		return;
+
+	macvlan = netdev_priv(dev);
+	stats = this_cpu_ptr(macvlan->pcpu_stats);
+	u64_stats_update_begin(&stats->syncp);
+	u64_stats_add(&stats->rx_packets, offl_stats->rx_packets);
+	u64_stats_add(&stats->rx_bytes, offl_stats->rx_bytes);
+	u64_stats_add(&stats->tx_packets, offl_stats->tx_packets);
+	u64_stats_add(&stats->tx_bytes, offl_stats->tx_bytes);
+	/* Update multicast statistics */
+	if (unlikely(update_mcast_rx_stats)) {
+		u64_stats_add(&stats->rx_multicast, offl_stats->rx_packets);
+	}
+	u64_stats_update_end(&stats->syncp);
+}
+
 static void macvlan_dev_get_stats64(struct net_device *dev,
 				    struct rtnl_link_stats64 *stats)
 {
@@ -1465,6 +1491,7 @@ int macvlan_common_newlink(struct net *s
 	vlan->dev      = dev;
 	vlan->port     = port;
 	vlan->set_features = MACVLAN_FEATURES;
+	vlan->offload_stats_update = macvlan_dev_update_stats;
 
 	vlan->mode     = MACVLAN_MODE_VEPA;
 	if (data && data[IFLA_MACVLAN_MODE])
--- a/drivers/net/ppp/ppp_generic.c
+++ b/drivers/net/ppp/ppp_generic.c
@@ -64,6 +64,7 @@
 #include <net/slhc_vj.h>
 #include <linux/atomic.h>
 #include <linux/refcount.h>
+#include <linux/if_pppox.h>
 
 #include <linux/nsproxy.h>
 #include <net/net_namespace.h>
@@ -2986,6 +2987,20 @@ char *ppp_dev_name(struct ppp_channel *c
 	return name;
 }
 
+/* Return the PPP net device index */
+int ppp_dev_index(struct ppp_channel *chan)
+{
+	struct channel *pch = chan->ppp;
+	int ifindex = 0;
+
+	if (pch) {
+		read_lock_bh(&pch->upl);
+		if (pch->ppp && pch->ppp->dev)
+			ifindex = pch->ppp->dev->ifindex;
+		read_unlock_bh(&pch->upl);
+	}
+	return ifindex;
+}
 
 /*
  * Disconnect a channel from the generic layer.
@@ -3486,6 +3501,8 @@ ppp_connect_channel(struct channel *pch,
 	struct ppp_net *pn;
 	int ret = -ENXIO;
 	int hdrlen;
+	int ppp_proto;
+	int version;
 	int notify = 0;
 
 	pn = ppp_pernet(pch->chan_net);
@@ -3519,6 +3536,30 @@ ppp_connect_channel(struct channel *pch,
 	++ppp->n_channels;
 	pch->ppp = ppp;
 	refcount_inc(&ppp->file.refcnt);
+
+	ppp_proto = ppp_channel_get_protocol(pch->chan);
+	switch (ppp_proto) {
+	case PX_PROTO_OL2TP:
+		version = ppp_channel_get_proto_version(pch->chan);
+		switch (version) {
+		case 2:
+			ppp->dev->priv_flags_ext |= IFF_EXT_PPP_L2TPV2;
+			break;
+		case 3:
+			ppp->dev->priv_flags_ext |= IFF_EXT_PPP_L2TPV3;
+			break;
+		}
+
+		break;
+
+	case PX_PROTO_PPTP:
+		ppp->dev->priv_flags_ext |= IFF_EXT_PPP_PPTP;
+		break;
+
+	default:
+		break;
+	}
+
 	notify = 1;
 
 	ppp_unlock(ppp);
@@ -3723,6 +3764,32 @@ int ppp_is_multilink(struct net_device *
 }
 EXPORT_SYMBOL(ppp_is_multilink);
 
+/* __ppp_is_multilink()
+ *	Returns >0 if the device is a multilink PPP netdevice, 0 if not or < 0
+ *	if the device is not PPP. Caller should acquire ppp_lock before calling
+ *	this function
+ */
+int __ppp_is_multilink(struct net_device *dev)
+{
+	struct ppp *ppp;
+	unsigned int flags;
+
+	if (!dev)
+		return -1;
+
+	if (dev->type != ARPHRD_PPP)
+		return -1;
+
+	ppp = netdev_priv(dev);
+	flags = ppp->flags;
+
+	if (flags & SC_MULTILINK)
+		return 1;
+
+	return 0;
+}
+EXPORT_SYMBOL(__ppp_is_multilink);
+
 /* ppp_channel_get_protocol()
  *	Call this to obtain the underlying protocol of the PPP channel,
  *	e.g. PX_PROTO_OE
@@ -3743,6 +3810,18 @@ int ppp_channel_get_protocol(struct ppp_
 }
 EXPORT_SYMBOL(ppp_channel_get_protocol);
 
+/* ppp_channel_get_proto_version()
+ *	Call this to get channel protocol version
+ */
+int ppp_channel_get_proto_version(struct ppp_channel *chan)
+{
+	if (!chan->ops->get_channel_protocol_ver)
+		return -1;
+
+	return chan->ops->get_channel_protocol_ver(chan);
+}
+EXPORT_SYMBOL(ppp_channel_get_proto_version);
+
 /* ppp_channel_hold()
  *	Call this to hold a channel.
  *
@@ -3827,6 +3906,59 @@ int ppp_hold_channels(struct net_device
 }
 EXPORT_SYMBOL(ppp_hold_channels);
 
+/* __ppp_hold_channels()
+ *	Returns the PPP channels of the PPP device, storing each one
+ *	into channels[].
+ *
+ * channels[] has chan_sz elements.
+ * This function returns the number of channels stored, up to chan_sz.
+ * It will return < 0 if the device is not PPP.
+ *
+ * You MUST acquire ppp_lock and  release the channels using
+ * ppp_release_channels().
+ */
+int __ppp_hold_channels(struct net_device *dev, struct ppp_channel *channels[],
+			unsigned int chan_sz)
+{
+	struct ppp *ppp;
+	int c;
+	struct channel *pch;
+
+	if (!dev)
+		return -1;
+
+	if (dev->type != ARPHRD_PPP)
+		return -1;
+
+	ppp = netdev_priv(dev);
+
+	c = 0;
+	list_for_each_entry(pch, &ppp->channels, clist) {
+		struct ppp_channel *chan;
+
+		if (!pch->chan) {
+			/* Channel is going / gone away*/
+			continue;
+		}
+		if (c == chan_sz) {
+			/* No space to record channel */
+			return c;
+		}
+
+		/* Hold the channel, if supported */
+		chan = pch->chan;
+		if (!chan->ops->hold)
+			continue;
+
+		chan->ops->hold(chan);
+
+		/* Record the channel */
+		channels[c++] = chan;
+	}
+	return c;
+}
+EXPORT_SYMBOL(__ppp_hold_channels);
+
 /* ppp_release_channels()
  *	Releases channels
  */
@@ -3843,6 +3975,28 @@ void ppp_release_channels(struct ppp_cha
 }
 EXPORT_SYMBOL(ppp_release_channels);
 
+/* Check if ppp xmit lock is on hold */
+bool ppp_is_xmit_locked(struct net_device *dev)
+{
+	struct ppp *ppp;
+
+	if (!dev)
+		return false;
+
+	if (dev->type != ARPHRD_PPP)
+		return false;
+
+	ppp = netdev_priv(dev);
+	if (!ppp)
+		return false;
+
+	if (spin_is_locked(&(ppp)->wlock))
+		return true;
+
+	return false;
+}
+EXPORT_SYMBOL(ppp_is_xmit_locked);
+
 /* Module/initialization stuff */
 
 module_init(ppp_init);
@@ -3854,6 +4008,7 @@ EXPORT_SYMBOL(ppp_unregister_channel);
 EXPORT_SYMBOL(ppp_channel_index);
 EXPORT_SYMBOL(ppp_unit_number);
 EXPORT_SYMBOL(ppp_dev_name);
+EXPORT_SYMBOL(ppp_dev_index);
 EXPORT_SYMBOL(ppp_input);
 EXPORT_SYMBOL(ppp_input_error);
 EXPORT_SYMBOL(ppp_output_wakeup);
--- a/drivers/net/ppp/pptp.c
+++ b/drivers/net/ppp/pptp.c
@@ -806,9 +806,36 @@ void pptp_unregister_gre_seq_offload_cal
 }
 EXPORT_SYMBOL(pptp_unregister_gre_seq_offload_callback);
 
+/* pptp_hold_chan() */
+static void pptp_hold_chan(struct ppp_channel *chan)
+{
+	struct sock *sk = (struct sock *)chan->private;
+
+	sock_hold(sk);
+}
+
+/* pptp_release_chan() */
+static void pptp_release_chan(struct ppp_channel *chan)
+{
+	struct sock *sk = (struct sock *)chan->private;
+
+	sock_put(sk);
+}
+
+/* pptp_get_channel_protocol()
+ *     Return the protocol type of the PPTP over PPP protocol
+ */
+static int pptp_get_channel_protocol(struct ppp_channel *chan)
+{
+	return PX_PROTO_PPTP;
+}
+
 static const struct ppp_channel_ops pptp_chan_ops = {
 	.start_xmit = pptp_xmit,
 	.ioctl      = pptp_ppp_ioctl,
+	.get_channel_protocol = pptp_get_channel_protocol,
+	.hold = pptp_hold_chan,
+	.release = pptp_release_chan,
 };
 
 static struct proto pptp_sk_proto __read_mostly = {
--- a/include/linux/if_bridge.h
+++ b/include/linux/if_bridge.h
@@ -73,6 +73,7 @@ extern struct net_device *br_port_dev_ge
 					  struct sk_buff *skb,
 					  unsigned int cookie);
 extern void br_refresh_fdb_entry(struct net_device *dev, const char *addr);
+extern void br_fdb_entry_refresh(struct net_device *dev, const char *addr, __u16 vid);
 extern void br_dev_update_stats(struct net_device *dev,
 				struct rtnl_link_stats64 *nlstats);
 extern struct net_bridge_fdb_entry *br_fdb_has_entry(struct net_device *dev,
@@ -80,6 +81,7 @@ extern struct net_bridge_fdb_entry *br_f
 						     __u16 vid);
 extern void br_fdb_update_register_notify(struct notifier_block *nb);
 extern void br_fdb_update_unregister_notify(struct notifier_block *nb);
+extern bool br_is_hairpin_enabled(struct net_device *dev);
 
 #if IS_ENABLED(CONFIG_BRIDGE) && IS_ENABLED(CONFIG_BRIDGE_IGMP_SNOOPING)
 int br_multicast_list_adjacent(struct net_device *dev,
--- a/include/linux/if_macvlan.h
+++ b/include/linux/if_macvlan.h
@@ -15,6 +15,11 @@ struct macvlan_port;
 #define MACVLAN_MC_FILTER_BITS	8
 #define MACVLAN_MC_FILTER_SZ	(1 << MACVLAN_MC_FILTER_BITS)
 
+/*
+ * Callback for updating interface statistics for macvlan flows offloaded from host CPU.
+ */
+typedef void (*macvlan_offload_stats_update_cb_t)(struct net_device *dev, struct rtnl_link_stats64 *stats, bool update_mcast_rx_stats);
+
 struct macvlan_dev {
 	struct net_device	*dev;
 	struct list_head	list;
@@ -34,6 +39,7 @@ struct macvlan_dev {
 #ifdef CONFIG_NET_POLL_CONTROLLER
 	struct netpoll		*netpoll;
 #endif
+	macvlan_offload_stats_update_cb_t	offload_stats_update;
 };
 
 static inline void macvlan_count_rx(const struct macvlan_dev *vlan,
@@ -67,6 +73,24 @@ extern void macvlan_dellink(struct net_d
 extern int macvlan_link_register(struct rtnl_link_ops *ops);
 
 #if IS_ENABLED(CONFIG_MACVLAN)
+static inline void
+macvlan_offload_stats_update(struct net_device *dev,
+			     struct rtnl_link_stats64 *stats,
+			     bool update_mcast_rx_stats)
+{
+	struct macvlan_dev *macvlan = netdev_priv(dev);
+
+	macvlan->offload_stats_update(dev, stats, update_mcast_rx_stats);
+}
+
+static inline enum
+macvlan_mode macvlan_get_mode(struct net_device *dev)
+{
+	struct macvlan_dev *macvlan = netdev_priv(dev);
+
+	return macvlan->mode;
+}
+
 static inline struct net_device *
 macvlan_dev_real_dev(const struct net_device *dev)
 {
--- a/include/linux/if_pppol2tp.h
+++ b/include/linux/if_pppol2tp.h
@@ -14,4 +14,27 @@
 #include <linux/in6.h>
 #include <uapi/linux/if_pppol2tp.h>
 
+/*
+ * Holds L2TP channel info
+ */
+struct  pppol2tp_common_addr {
+	int tunnel_version;				/* v2 or v3 */
+	__u32 local_tunnel_id, remote_tunnel_id;	/* tunnel id */
+	__u32 local_session_id, remote_session_id;	/* session id */
+	struct sockaddr_in local_addr, remote_addr; /* ip address and port */
+};
+
+/*
+ * L2TP channel operations
+ */
+struct pppol2tp_channel_ops {
+	struct ppp_channel_ops ops; /* ppp channel ops */
+};
+
+/*
+ * exported function which calls pppol2tp channel's get addressing
+ * function
+ */
+extern int pppol2tp_channel_addressing_get(struct ppp_channel *,
+					   struct pppol2tp_common_addr *);
 #endif
--- a/include/linux/if_vlan.h
+++ b/include/linux/if_vlan.h
@@ -141,7 +141,11 @@ extern struct net_device *__vlan_find_de
 extern int vlan_for_each(struct net_device *dev,
 			 int (*action)(struct net_device *dev, int vid,
 				       void *arg), void *arg);
+extern void __vlan_dev_update_accel_stats(struct net_device *dev,
+				    struct rtnl_link_stats64 *stats);
+
 extern struct net_device *vlan_dev_real_dev(const struct net_device *dev);
+extern struct net_device *vlan_dev_next_dev(const struct net_device *dev);
 extern u16 vlan_dev_vlan_id(const struct net_device *dev);
 extern __be16 vlan_dev_vlan_proto(const struct net_device *dev);
 
@@ -231,6 +235,12 @@ extern void vlan_vids_del_by_dev(struct
 extern bool vlan_uses_dev(const struct net_device *dev);
 
 #else
+static inline void __vlan_dev_update_accel_stats(struct net_device *dev,
+					   struct rtnl_link_stats64 *stats)
+{
+
+}
+
 static inline struct net_device *
 __vlan_find_dev_deep_rcu(struct net_device *real_dev,
 		     __be16 vlan_proto, u16 vlan_id)
--- a/include/linux/mroute.h
+++ b/include/linux/mroute.h
@@ -85,4 +85,42 @@ struct rtmsg;
 int ipmr_get_route(struct net *net, struct sk_buff *skb,
 		   __be32 saddr, __be32 daddr,
 		   struct rtmsg *rtm, u32 portid);
+
+#define IPMR_MFC_EVENT_UPDATE   1
+#define IPMR_MFC_EVENT_DELETE   2
+
+/*
+ * Callback to registered modules in the event of updates to a multicast group
+ */
+typedef void (*ipmr_mfc_event_offload_callback_t)(__be32 origin, __be32 group,
+						  u32 max_dest_dev,
+						  u32 dest_dev_idx[],
+						  u8 op);
+
+/*
+ * Register the callback used to inform offload modules when updates occur to
+ * MFC. The callback is registered by offload modules
+ */
+extern bool ipmr_register_mfc_event_offload_callback(
+			ipmr_mfc_event_offload_callback_t mfc_offload_cb);
+
+/*
+ * De-Register the callback used to inform offload modules when updates occur
+ * to MFC
+ */
+extern void ipmr_unregister_mfc_event_offload_callback(void);
+
+/*
+ * Find the destination interface list, given a multicast group and source
+ */
+extern int ipmr_find_mfc_entry(struct net *net, __be32 origin, __be32 group,
+				 u32 max_dst_cnt, u32 dest_dev[]);
+
+/*
+ * Out-of-band multicast statistics update for flows that are offloaded from
+ * Linux
+ */
+extern int ipmr_mfc_stats_update(struct net *net, __be32 origin, __be32 group,
+				 u64 pkts_in, u64 bytes_in,
+				 u64 pkts_out, u64 bytes_out);
 #endif
--- a/include/linux/mroute6.h
+++ b/include/linux/mroute6.h
@@ -93,10 +93,51 @@ struct mfc6_cache {
 
 #define MFC_ASSERT_THRESH (3*HZ)		/* Maximal freq. of asserts */
 
+#define IP6MR_MFC_EVENT_UPDATE   1
+#define IP6MR_MFC_EVENT_DELETE   2
+
 struct rtmsg;
 extern int ip6mr_get_route(struct net *net, struct sk_buff *skb,
 			   struct rtmsg *rtm, u32 portid);
 
+/*
+ * Callback to registered modules in the event of updates to a multicast group
+ */
+typedef void (*ip6mr_mfc_event_offload_callback_t)(struct in6_addr *origin,
+						   struct in6_addr *group,
+						   u32 max_dest_dev,
+						   u32 dest_dev_idx[],
+						   uint8_t op);
+
+/*
+ * Register the callback used to inform offload modules when updates occur
+ * to MFC. The callback is registered by offload modules
+ */
+extern bool ip6mr_register_mfc_event_offload_callback(
+			ip6mr_mfc_event_offload_callback_t mfc_offload_cb);
+
+/*
+ * De-Register the callback used to inform offload modules when updates occur
+ * to MFC
+ */
+extern void ip6mr_unregister_mfc_event_offload_callback(void);
+
+/*
+ * Find the destination interface list given a multicast group and source
+ */
+extern int ip6mr_find_mfc_entry(struct net *net, struct in6_addr *origin,
+				struct in6_addr *group, u32 max_dst_cnt,
+				u32 dest_dev[]);
+
+/*
+ * Out-of-band multicast statistics update for flows that are offloaded from
+ * Linux
+ */
+extern int ip6mr_mfc_stats_update(struct net *net, struct in6_addr *origin,
+				  struct in6_addr *group, uint64_t pkts_in,
+				  uint64_t bytes_in, uint64_t pkts_out,
+				  uint64_t bytes_out);
+
 #ifdef CONFIG_IPV6_MROUTE
 bool mroute6_is_socket(struct net *net, struct sk_buff *skb);
 extern int ip6mr_sk_done(struct sock *sk);
--- a/include/linux/netdevice.h
+++ b/include/linux/netdevice.h
@@ -1699,6 +1699,7 @@ enum netdev_extra_priv_flags {
  * @IFF_EXT_GRE_V4_TAP: device is a GRE IPv4 TAP device
  * @IFF_EXT_GRE_V6_TAP: device is a GRE IPv6 TAP device
  * @IFF_EXT_IFB: device is an IFB device
+ * @IFF_EXT_MAPT: device is an MAPT device
  */
 enum netdev_priv_flags_ext {
 	IFF_EXT_TUN_TAP			= 1<<0,
@@ -1707,7 +1708,10 @@ enum netdev_priv_flags_ext {
 	IFF_EXT_PPP_PPTP		= 1<<3,
 	IFF_EXT_GRE_V4_TAP		= 1<<4,
 	IFF_EXT_GRE_V6_TAP		= 1<<5,
-	IFF_EXT_IFB			= 1<<6,
+	IFF_EXT_IFB				= 1<<6,
+	IFF_EXT_MAPT			= 1<<7,
+	IFF_EXT_HW_NO_OFFLOAD	= 1<<8,
+	IFF_EXT_ETH_L2TPV3		= 1<<9,
 };
 
 #define IFF_802_1Q_VLAN			IFF_802_1Q_VLAN
--- a/include/linux/ppp_channel.h
+++ b/include/linux/ppp_channel.h
@@ -19,6 +19,7 @@
 #include <linux/skbuff.h>
 #include <linux/poll.h>
 #include <net/net_namespace.h>
+#include <linux/ppp_defs.h>
 #include <linux/notifier.h>
 
 /* PPP channel connection event types */
@@ -40,6 +41,8 @@ struct ppp_channel_ops {
 	 * the channel subtype
 	 */
 	int (*get_channel_protocol)(struct ppp_channel *);
+	/* Get channel protocol version */
+	int (*get_channel_protocol_ver)(struct ppp_channel *);
 	/* Hold the channel from being destroyed */
 	void (*hold)(struct ppp_channel *);
 	/* Release hold on the channel */
@@ -63,6 +66,9 @@ struct ppp_channel {
  */
 extern int ppp_channel_get_protocol(struct ppp_channel *);
 
+/* Call this get protocol version */
+extern int ppp_channel_get_proto_version(struct ppp_channel *);
+
 /* Call this to hold a channel */
 extern bool ppp_channel_hold(struct ppp_channel *);
 
@@ -80,9 +86,20 @@ extern int ppp_hold_channels(struct net_
 
 bool ppp_is_cp_enabled(struct net_device *dev);
 
+/* Test if ppp xmit lock is locked */
+extern bool ppp_is_xmit_locked(struct net_device *dev);
+
+/* Hold PPP channels for the PPP device */
+extern int __ppp_hold_channels(struct net_device *dev,
+			       struct ppp_channel *channels[],
+			       unsigned int chan_sz);
+
 /* Test if the ppp device is a multi-link ppp device */
 extern int ppp_is_multilink(struct net_device *dev);
 
+/* Test if the ppp device is a multi-link ppp device */
+extern int __ppp_is_multilink(struct net_device *dev);
+
 /* Update statistics of the PPP net_device by incrementing related
  * statistics field value with corresponding parameter
  */
@@ -115,6 +132,9 @@ extern void ppp_unregister_channel(struc
 /* Get the channel number for a channel */
 extern int ppp_channel_index(struct ppp_channel *);
 
+/* Get the device index  associated with a channel, or 0, if none */
+extern int ppp_dev_index(struct ppp_channel *);
+
 /* Get the unit number associated with a channel, or -1 if none */
 extern int ppp_unit_number(struct ppp_channel *);
 
--- a/include/net/ip6_route.h
+++ b/include/net/ip6_route.h
@@ -211,6 +211,9 @@ void rt6_multipath_rebalance(struct fib6
 void rt6_uncached_list_add(struct rt6_info *rt);
 void rt6_uncached_list_del(struct rt6_info *rt);
 
+int rt6_register_notifier(struct notifier_block *nb);
+int rt6_unregister_notifier(struct notifier_block *nb);
+
 static inline const struct rt6_info *skb_rt6_info(const struct sk_buff *skb)
 {
 	const struct dst_entry *dst = skb_dst(skb);
--- a/include/net/neighbour.h
+++ b/include/net/neighbour.h
@@ -241,6 +241,11 @@ static inline int neigh_parms_family(str
 	return p->tbl->family;
 }
 
+struct neigh_mac_update {
+	unsigned char old_mac[ALIGN(MAX_ADDR_LEN, sizeof(unsigned long))];
+	unsigned char update_mac[ALIGN(MAX_ADDR_LEN, sizeof(unsigned long))];
+};
+
 #define NEIGH_PRIV_ALIGN	sizeof(long long)
 #define NEIGH_ENTRY_SIZE(size)	ALIGN((size), NEIGH_PRIV_ALIGN)
 
@@ -369,6 +374,9 @@ int neigh_xmit(int fam, struct net_devic
 			      int (*cb)(struct neighbour *));
 int neigh_xmit(int fam, struct net_device *, const void *, struct sk_buff *);
 
+extern void neigh_mac_update_register_notify(struct notifier_block *nb);
+extern void neigh_mac_update_unregister_notify(struct notifier_block *nb);
+
 struct neigh_seq_state {
 	struct seq_net_private p;
 	struct neigh_table *tbl;
--- a/include/net/netfilter/nf_conntrack_ecache.h
+++ b/include/net/netfilter/nf_conntrack_ecache.h
@@ -83,14 +83,52 @@ struct nf_ct_event_notifier {
 	int (*exp_event)(unsigned int events, const struct nf_exp_event *item);
 };
 
+#ifdef CONFIG_NF_CONNTRACK_CHAIN_EVENTS
+extern int nf_conntrack_register_notifier(struct net *net,
+					  struct notifier_block *nb);
+extern int nf_conntrack_unregister_notifier(struct net *net,
+					    struct notifier_block *nb);
+static inline int
+nf_conntrack_eventmask_report(unsigned int eventmask,
+			      struct nf_conn *ct,
+			      u32 portid,
+			      int report)
+{
+	struct nf_conntrack_ecache *e;
+	struct net *net = nf_ct_net(ct);
+
+	e = nf_ct_ecache_find(ct);
+	if (e == NULL)
+		return 0;
+
+	if (nf_ct_is_confirmed(ct)) {
+		struct nf_ct_event item = {
+			.ct = ct,
+			.portid = e->portid ? e->portid : portid,
+			.report = report
+		};
+		/* This is a resent of a destroy event? If so, skip missed */
+		unsigned long missed = e->portid ? 0 : e->missed;
+
+		if (!((eventmask | missed) & e->ctmask))
+			return 0;
+
+		atomic_notifier_call_chain(&net->ct.nf_conntrack_chain,
+					   eventmask | missed, &item);
+	}
+
+	return 0;
+}
+#else
+
 void nf_conntrack_register_notifier(struct net *net,
 				   const struct nf_ct_event_notifier *nb);
 void nf_conntrack_unregister_notifier(struct net *net);
-
-void nf_ct_deliver_cached_events(struct nf_conn *ct);
 int nf_conntrack_eventmask_report(unsigned int eventmask, struct nf_conn *ct,
 				  u32 portid, int report);
-
+#endif
+
+void nf_ct_deliver_cached_events(struct nf_conn *ct);
 bool nf_ct_ecache_ext_add(struct nf_conn *ct, u16 ctmask, u16 expmask, gfp_t gfp);
 #else
 
@@ -111,11 +150,13 @@ static inline void
 nf_conntrack_event_cache(enum ip_conntrack_events event, struct nf_conn *ct)
 {
 #ifdef CONFIG_NF_CONNTRACK_EVENTS
-	struct net *net = nf_ct_net(ct);
 	struct nf_conntrack_ecache *e;
+#ifndef CONFIG_NF_CONNTRACK_CHAIN_EVENTS
+	struct net *net = nf_ct_net(ct);
 
 	if (!rcu_access_pointer(net->ct.nf_conntrack_event_cb))
 		return;
+#endif
 
 	e = nf_ct_ecache_find(ct);
 	if (e == NULL)
--- a/include/net/netfilter/nf_conntrack_timeout.h
+++ b/include/net/netfilter/nf_conntrack_timeout.h
@@ -123,5 +123,6 @@ static inline void nf_ct_destroy_timeout
 
 extern const struct nf_ct_timeout_hooks __rcu *nf_ct_timeout_hook;
 #endif
+extern unsigned int *udp_get_timeouts(struct net *net);
 
 #endif /* _NF_CONNTRACK_TIMEOUT_H */
--- a/include/net/netns/conntrack.h
+++ b/include/net/netns/conntrack.h
@@ -113,6 +113,9 @@ struct netns_ct {
 	u8			sysctl_tstamp;
 	u8			sysctl_checksum;
 
+#ifdef CONFIG_NF_CONNTRACK_CHAIN_EVENTS
+	struct atomic_notifier_head nf_conntrack_chain;
+#endif
 	struct ip_conntrack_stat __percpu *stat;
 	struct nf_ct_event_notifier __rcu *nf_conntrack_event_cb;
 	struct nf_ip_net	nf_ct_proto;
--- a/include/net/route.h
+++ b/include/net/route.h
@@ -234,6 +234,9 @@ struct rtable *rt_dst_alloc(struct net_d
 			    unsigned int flags, u16 type, bool noxfrm);
 struct rtable *rt_dst_clone(struct net_device *dev, struct rtable *rt);
 
+int ip_rt_register_notifier(struct notifier_block *nb);
+int ip_rt_unregister_notifier(struct notifier_block *nb);
+
 struct in_ifaddr;
 void fib_add_ifaddr(struct in_ifaddr *);
 void fib_del_ifaddr(struct in_ifaddr *, struct in_ifaddr *);
--- a/include/uapi/linux/in.h
+++ b/include/uapi/linux/in.h
@@ -62,6 +62,8 @@ enum {
 #define IPPROTO_MTP		IPPROTO_MTP
   IPPROTO_BEETPH = 94,		/* IP option pseudo header for BEET	*/
 #define IPPROTO_BEETPH		IPPROTO_BEETPH
+  IPPROTO_ETHERIP = 97,		/* ETHERIP protocol number		*/
+#define IPPROTO_ETHERIP		IPPROTO_ETHERIP
   IPPROTO_ENCAP = 98,		/* Encapsulation Header			*/
 #define IPPROTO_ENCAP		IPPROTO_ENCAP
   IPPROTO_PIM = 103,		/* Protocol Independent Multicast	*/
@@ -332,7 +334,7 @@ struct sockaddr_in {
 #endif
 
 /* <asm/byteorder.h> contains the htonl type stuff.. */
-#include <asm/byteorder.h> 
+#include <asm/byteorder.h>
 
 
 #endif /* _UAPI_LINUX_IN_H */
--- a/net/8021q/vlan_core.c
+++ b/net/8021q/vlan_core.c
@@ -72,6 +72,26 @@ bool vlan_do_receive(struct sk_buff **sk
 	return true;
 }
 
+/* Update the VLAN device with statistics from network offload engines */
+void __vlan_dev_update_accel_stats(struct net_device *dev,
+				   struct rtnl_link_stats64 *nlstats)
+{
+	struct vlan_pcpu_stats *stats;
+
+	if (!is_vlan_dev(dev))
+		return;
+
+	stats = per_cpu_ptr(vlan_dev_priv(dev)->vlan_pcpu_stats, 0);
+
+	u64_stats_update_begin(&stats->syncp);
+	u64_stats_add(&stats->rx_packets, nlstats->rx_packets);
+	u64_stats_add(&stats->rx_bytes, nlstats->rx_bytes);
+	u64_stats_add(&stats->tx_packets, nlstats->tx_packets);
+	u64_stats_add(&stats->tx_bytes, nlstats->tx_bytes);
+	u64_stats_update_end(&stats->syncp);
+}
+EXPORT_SYMBOL(__vlan_dev_update_accel_stats);
+
 /* Must be invoked with rcu_read_lock. */
 struct net_device *__vlan_find_dev_deep_rcu(struct net_device *dev,
 					__be16 vlan_proto, u16 vlan_id)
@@ -110,6 +130,13 @@ struct net_device *vlan_dev_real_dev(con
 }
 EXPORT_SYMBOL(vlan_dev_real_dev);
 
+/* Caller is responsible to hold the reference of the returned device */
+struct net_device *vlan_dev_next_dev(const struct net_device *dev)
+{
+	return vlan_dev_priv(dev)->real_dev;
+}
+EXPORT_SYMBOL(vlan_dev_next_dev);
+
 u16 vlan_dev_vlan_id(const struct net_device *dev)
 {
 	return vlan_dev_priv(dev)->vlan_id;
--- a/net/bridge/br_fdb.c
+++ b/net/bridge/br_fdb.c
@@ -277,6 +277,24 @@ void br_refresh_fdb_entry(struct net_dev
 }
 EXPORT_SYMBOL_GPL(br_refresh_fdb_entry);
 
+/* Update timestamp of FDB entries for bridge packets being forwarded by offload engines */
+void br_fdb_entry_refresh(struct net_device *dev, const char *addr, __u16 vid)
+{
+	struct net_bridge_fdb_entry *fdb;
+	struct net_bridge_port *p = br_port_get_rcu(dev);
+
+	if (!p || p->state == BR_STATE_DISABLED)
+		return;
+
+	rcu_read_lock();
+	fdb = fdb_find_rcu(&p->br->fdb_hash_tbl, addr, vid);
+	if (likely(fdb)) {
+		fdb->updated = jiffies;
+	}
+	rcu_read_unlock();
+}
+EXPORT_SYMBOL_GPL(br_fdb_entry_refresh);
+
 /* Look up the MAC address in the device's bridge fdb table */
 struct net_bridge_fdb_entry *br_fdb_has_entry(struct net_device *dev,
 					      const char *addr, __u16 vid)
--- a/net/bridge/br_if.c
+++ b/net/bridge/br_if.c
@@ -880,3 +880,14 @@ void br_dev_update_stats(struct net_devi
 	u64_stats_update_end(&tstats->syncp);
 }
 EXPORT_SYMBOL_GPL(br_dev_update_stats);
+
+/* API to know if hairpin feature is enabled/disabled on this bridge port */
+bool br_is_hairpin_enabled(struct net_device *dev)
+{
+	struct net_bridge_port *port = br_port_get_check_rcu(dev);
+
+	if (likely(port))
+		return port->flags & BR_HAIRPIN_MODE;
+	return false;
+}
+EXPORT_SYMBOL_GPL(br_is_hairpin_enabled);
--- a/net/core/neighbour.c
+++ b/net/core/neighbour.c
@@ -1275,6 +1275,20 @@ static void neigh_update_hhs(struct neig
 	}
 }
 
+ATOMIC_NOTIFIER_HEAD(neigh_mac_update_notifier_list);
+
+void neigh_mac_update_register_notify(struct notifier_block *nb)
+{
+	atomic_notifier_chain_register(&neigh_mac_update_notifier_list, nb);
+}
+EXPORT_SYMBOL_GPL(neigh_mac_update_register_notify);
+
+void neigh_mac_update_unregister_notify(struct notifier_block *nb)
+{
+	atomic_notifier_chain_unregister(&neigh_mac_update_notifier_list, nb);
+}
+EXPORT_SYMBOL_GPL(neigh_mac_update_unregister_notify);
+
 /* Generic update routine.
    -- lladdr is new lladdr or NULL, if it is not supplied.
    -- new    is new state.
@@ -1303,6 +1317,7 @@ static int __neigh_update(struct neighbo
 	struct net_device *dev;
 	int err, notify = 0;
 	u8 old;
+	struct neigh_mac_update nmu;
 
 	trace_neigh_update(neigh, lladdr, new, flags, nlmsg_pid);
 
@@ -1312,6 +1327,8 @@ static int __neigh_update(struct neighbo
 	old    = neigh->nud_state;
 	err    = -EPERM;
 
+	memset(&nmu, 0, sizeof(struct neigh_mac_update));
+
 	if (neigh->dead) {
 		NL_SET_ERR_MSG(extack, "Neighbor entry is now dead");
 		new = old;
@@ -1353,7 +1370,10 @@ static int __neigh_update(struct neighbo
 		   and a new address is proposed:
 		   - compare new & old
 		   - if they are different, check override flag
+		   - copy old and new addresses for neigh update notification
 		 */
+		memcpy(nmu.old_mac, neigh->ha, dev->addr_len);
+		memcpy(nmu.update_mac, lladdr, dev->addr_len);
 		if ((old & NUD_VALID) &&
 		    !memcmp(lladdr, neigh->ha, dev->addr_len))
 			lladdr = neigh->ha;
@@ -1476,8 +1496,11 @@ out:
 		neigh_update_gc_list(neigh);
 	if (managed_update)
 		neigh_update_managed_list(neigh);
-	if (notify)
+	if (notify) {
 		neigh_update_notify(neigh, nlmsg_pid);
+		atomic_notifier_call_chain(&neigh_mac_update_notifier_list, 0,
+					   (struct neigh_mac_update *)&nmu);
+	}
 	trace_neigh_update_done(neigh, err);
 	return err;
 }
--- a/net/ipv4/fib_trie.c
+++ b/net/ipv4/fib_trie.c
@@ -1206,6 +1206,9 @@ static bool fib_valid_key_len(u32 key, u
 static void fib_remove_alias(struct trie *t, struct key_vector *tp,
 			     struct key_vector *l, struct fib_alias *old);
 
+/* Define route change notification chain. */
+static BLOCKING_NOTIFIER_HEAD(iproute_chain);
+
 /* Caller must hold RTNL. */
 int fib_table_insert(struct net *net, struct fib_table *tb,
 		     struct fib_config *cfg, struct netlink_ext_ack *extack)
@@ -1398,6 +1401,8 @@ int fib_table_insert(struct net *net, st
 	rtmsg_fib(RTM_NEWROUTE, htonl(key), new_fa, plen, new_fa->tb_id,
 		  &cfg->fc_nlinfo, nlflags);
 succeeded:
+	blocking_notifier_call_chain(&iproute_chain,
+				     RTM_NEWROUTE, fi);
 	return 0;
 
 out_remove_new_fa:
@@ -1769,6 +1774,8 @@ int fib_table_delete(struct net *net, st
 	if (fa_to_delete->fa_state & FA_S_ACCESSED)
 		rt_cache_flush(cfg->fc_nlinfo.nl_net);
 
+	blocking_notifier_call_chain(&iproute_chain,
+				     RTM_DELROUTE, fa_to_delete->fa_info);
 	fib_release_info(fa_to_delete->fa_info);
 	alias_free_mem_rcu(fa_to_delete);
 	return 0;
@@ -2401,6 +2408,18 @@ void __init fib_trie_init(void)
 					   0, SLAB_PANIC | SLAB_ACCOUNT, NULL);
 }
 
+int ip_rt_register_notifier(struct notifier_block *nb)
+{
+	return blocking_notifier_chain_register(&iproute_chain, nb);
+}
+EXPORT_SYMBOL(ip_rt_register_notifier);
+
+int ip_rt_unregister_notifier(struct notifier_block *nb)
+{
+	return blocking_notifier_chain_unregister(&iproute_chain, nb);
+}
+EXPORT_SYMBOL(ip_rt_unregister_notifier);
+
 struct fib_table *fib_trie_table(u32 id, struct fib_table *alias)
 {
 	struct fib_table *tb;
--- a/net/ipv4/ipmr.c
+++ b/net/ipv4/ipmr.c
@@ -84,6 +84,9 @@ static DEFINE_RWLOCK(mrt_lock);
 /* Special spinlock for queue of unresolved entries */
 static DEFINE_SPINLOCK(mfc_unres_lock);
 
+/* spinlock for offload */
+static DEFINE_SPINLOCK(lock);
+
 /* We return to original Alan's scheme. Hash table of resolved
  * entries is changed only in process context and protected
  * with weak lock mrt_lock. Queue of unresolved entries is protected
@@ -107,6 +110,9 @@ static void mroute_netlink_event(struct
 static void igmpmsg_netlink_event(const struct mr_table *mrt, struct sk_buff *pkt);
 static void mroute_clean_tables(struct mr_table *mrt, int flags);
 static void ipmr_expire_process(struct timer_list *t);
+static struct mfc_cache *ipmr_cache_find(struct mr_table *mrt, __be32 origin,
+					 __be32 mcastgrp);
+static ipmr_mfc_event_offload_callback_t __rcu ipmr_mfc_event_offload_callback;
 
 #ifdef CONFIG_IP_MROUTE_MULTIPLE_TABLES
 #define ipmr_for_each_table(mrt, net)					\
@@ -222,6 +228,78 @@ static int ipmr_rule_fill(struct fib_rul
 	return 0;
 }
 
+/* ipmr_sync_entry_update()
+ * Call the registered offload callback to report an update to a multicast
+ * route entry. The callback receives the list of destination interfaces and
+ * the interface count
+ */
+static void ipmr_sync_entry_update(struct mr_table *mrt,
+				   struct mfc_cache *cache)
+{
+	int vifi, dest_if_count = 0;
+	u32 dest_dev[MAXVIFS];
+	__be32  origin;
+	__be32  group;
+	ipmr_mfc_event_offload_callback_t offload_update_cb_f;
+
+	memset(dest_dev, 0, sizeof(dest_dev));
+
+	origin = cache->mfc_origin;
+	group = cache->mfc_mcastgrp;
+
+	spin_lock(&mrt_lock);
+	for (vifi = 0; vifi < cache->_c.mfc_un.res.maxvif; vifi++) {
+		if (!((cache->_c.mfc_un.res.ttls[vifi] > 0) &&
+		      (cache->_c.mfc_un.res.ttls[vifi] < 255))) {
+			continue;
+		}
+		if (dest_if_count == MAXVIFS) {
+			spin_unlock(&mrt_lock);
+			return;
+		}
+
+		if (!VIF_EXISTS(mrt, vifi)) {
+			spin_unlock(&mrt_lock);
+			return;
+		}
+		dest_dev[dest_if_count] = mrt->vif_table[vifi].dev->ifindex;
+		dest_if_count++;
+	}
+	spin_unlock(&mrt_lock);
+
+	rcu_read_lock();
+	offload_update_cb_f = rcu_dereference(ipmr_mfc_event_offload_callback);
+
+	if (!offload_update_cb_f) {
+		rcu_read_unlock();
+		return;
+	}
+
+	offload_update_cb_f(group, origin, dest_if_count, dest_dev,
+			    IPMR_MFC_EVENT_UPDATE);
+	rcu_read_unlock();
+}
+
+/* ipmr_sync_entry_delete()
+ * Call the registered offload callback to inform of a multicast route entry
+ * delete event
+ */
+static void ipmr_sync_entry_delete(u32 origin, u32 group)
+{
+	ipmr_mfc_event_offload_callback_t offload_update_cb_f;
+
+	rcu_read_lock();
+	offload_update_cb_f = rcu_dereference(ipmr_mfc_event_offload_callback);
+
+	if (!offload_update_cb_f) {
+		rcu_read_unlock();
+		return;
+	}
+
+	offload_update_cb_f(group, origin, 0, NULL, IPMR_MFC_EVENT_DELETE);
+	rcu_read_unlock();
+}
+
 static const struct fib_rules_ops __net_initconst ipmr_rules_ops_template = {
 	.family		= RTNL_FAMILY_IPMR,
 	.rule_size	= sizeof(struct ipmr_rule),
@@ -236,6 +314,154 @@ static const struct fib_rules_ops __net_
 	.owner		= THIS_MODULE,
 };
 
+/* ipmr_register_mfc_event_offload_callback()
+ * Register the IPv4 Multicast update offload callback with IPMR
+ */
+bool ipmr_register_mfc_event_offload_callback(
+		ipmr_mfc_event_offload_callback_t mfc_offload_cb)
+{
+	ipmr_mfc_event_offload_callback_t offload_update_cb_f;
+
+	rcu_read_lock();
+	offload_update_cb_f = rcu_dereference(ipmr_mfc_event_offload_callback);
+
+	if (offload_update_cb_f) {
+		rcu_read_unlock();
+		return false;
+	}
+	rcu_read_unlock();
+
+	spin_lock(&lock);
+	rcu_assign_pointer(ipmr_mfc_event_offload_callback, mfc_offload_cb);
+	spin_unlock(&lock);
+	synchronize_rcu();
+	return true;
+}
+EXPORT_SYMBOL(ipmr_register_mfc_event_offload_callback);
+
+/* ipmr_unregister_mfc_event_offload_callback()
+ * De-register the IPv4 Multicast update offload callback with IPMR
+ */
+void ipmr_unregister_mfc_event_offload_callback(void)
+{
+	spin_lock(&lock);
+	rcu_assign_pointer(ipmr_mfc_event_offload_callback, NULL);
+	spin_unlock(&lock);
+	synchronize_rcu();
+}
+EXPORT_SYMBOL(ipmr_unregister_mfc_event_offload_callback);
+
+/* ipmr_find_mfc_entry()
+ * Returns destination interface list for a particular multicast flow, and
+ * the number of interfaces in the list
+ */
+int ipmr_find_mfc_entry(struct net *net, __be32 origin, __be32 group,
+			u32 max_dest_cnt, u32 dest_dev[])
+{
+	int vifi, dest_if_count = 0;
+	struct mr_table *mrt;
+	struct mfc_cache *cache;
+
+	mrt = ipmr_get_table(net, RT_TABLE_DEFAULT);
+	if (!mrt)
+		return -ENOENT;
+
+	rcu_read_lock();
+	cache = ipmr_cache_find(mrt, origin, group);
+	if (!cache) {
+		rcu_read_unlock();
+		return -ENOENT;
+	}
+
+	spin_lock(&mrt_lock);
+	for (vifi = 0; vifi < cache->_c.mfc_un.res.maxvif; vifi++) {
+		if (!((cache->_c.mfc_un.res.ttls[vifi] > 0) &&
+		      (cache->_c.mfc_un.res.ttls[vifi] < 255))) {
+			continue;
+		}
+
+		/* We have another valid destination interface entry. Check if
+		 * the number of the destination interfaces for the route is
+		 * exceeding the size of the array given to us
+		 */
+		if (dest_if_count == max_dest_cnt) {
+			spin_unlock(&mrt_lock);
+			rcu_read_unlock();
+			return -EINVAL;
+		}
+
+		if (!VIF_EXISTS(mrt, vifi)) {
+			spin_unlock(&mrt_lock);
+			rcu_read_unlock();
+			return -EINVAL;
+		}
+
+		dest_dev[dest_if_count] = mrt->vif_table[vifi].dev->ifindex;
+		dest_if_count++;
+	}
+	spin_unlock(&mrt_lock);
+	rcu_read_unlock();
+
+	return dest_if_count;
+}
+EXPORT_SYMBOL(ipmr_find_mfc_entry);
+
+/* ipmr_mfc_stats_update()
+ * Update the MFC/VIF statistics for offloaded flows
+ */
+int ipmr_mfc_stats_update(struct net *net, __be32 origin, __be32 group,
+			  u64 pkts_in, u64 bytes_in,
+			  u64 pkts_out, u64 bytes_out)
+{
+	int vif, vifi;
+	struct mr_table *mrt;
+	struct mfc_cache *cache;
+
+	mrt = ipmr_get_table(net, RT_TABLE_DEFAULT);
+	if (!mrt)
+		return -ENOENT;
+
+	rcu_read_lock();
+	cache = ipmr_cache_find(mrt, origin, group);
+	if (!cache) {
+		rcu_read_unlock();
+		return -ENOENT;
+	}
+
+	vif = cache->_c.mfc_parent;
+
+	spin_lock(&mrt_lock);
+	if (!VIF_EXISTS(mrt, vif)) {
+		spin_unlock(&mrt_lock);
+		rcu_read_unlock();
+		return -EINVAL;
+	}
+
+	mrt->vif_table[vif].pkt_in += pkts_in;
+	mrt->vif_table[vif].bytes_in += bytes_in;
+	cache->_c.mfc_un.res.pkt  += pkts_out;
+	cache->_c.mfc_un.res.bytes += bytes_out;
+
+	for (vifi = cache->_c.mfc_un.res.minvif;
+			vifi < cache->_c.mfc_un.res.maxvif; vifi++) {
+		if ((cache->_c.mfc_un.res.ttls[vifi] > 0) &&
+		    (cache->_c.mfc_un.res.ttls[vifi] < 255)) {
+			if (!VIF_EXISTS(mrt, vifi)) {
+				spin_unlock(&mrt_lock);
+				rcu_read_unlock();
+				return -EINVAL;
+			}
+			mrt->vif_table[vifi].pkt_out += pkts_out;
+			mrt->vif_table[vifi].bytes_out += bytes_out;
+		}
+	}
+	spin_unlock(&mrt_lock);
+	rcu_read_unlock();
+
+	return 0;
+}
+EXPORT_SYMBOL(ipmr_mfc_stats_update);
+
 static int __net_init ipmr_rules_init(struct net *net)
 {
 	struct fib_rules_ops *ops;
@@ -1184,6 +1410,8 @@ static int ipmr_mfc_delete(struct mr_tab
 	call_ipmr_mfc_entry_notifiers(net, FIB_EVENT_ENTRY_DEL, c, mrt->id);
 	mroute_netlink_event(mrt, c, RTM_DELROUTE);
 	mr_cache_put(&c->_c);
+	/* Inform offload modules of the delete event */
+	ipmr_sync_entry_delete(c->mfc_origin, c->mfc_mcastgrp);
 
 	return 0;
 }
@@ -1214,6 +1442,8 @@ static int ipmr_mfc_add(struct net *net,
 		call_ipmr_mfc_entry_notifiers(net, FIB_EVENT_ENTRY_REPLACE, c,
 					      mrt->id);
 		mroute_netlink_event(mrt, c, RTM_NEWROUTE);
+		/* Inform offload modules of the update event */
+		ipmr_sync_entry_update(mrt, c);
 		return 0;
 	}
 
@@ -1274,6 +1504,7 @@ static void mroute_clean_tables(struct m
 	struct net *net = read_pnet(&mrt->net);
 	struct mr_mfc *c, *tmp;
 	struct mfc_cache *cache;
+	u32 origin, group;
 	LIST_HEAD(list);
 	int i;
 
@@ -1298,10 +1529,14 @@ static void mroute_clean_tables(struct m
 			rhltable_remove(&mrt->mfc_hash, &c->mnode, ipmr_rht_params);
 			list_del_rcu(&c->list);
 			cache = (struct mfc_cache *)c;
+			origin = cache->mfc_origin;
+			group = cache->mfc_mcastgrp;
 			call_ipmr_mfc_entry_notifiers(net, FIB_EVENT_ENTRY_DEL, cache,
 						      mrt->id);
 			mroute_netlink_event(mrt, cache, RTM_DELROUTE);
 			mr_cache_put(c);
+			/* Inform offload modules of the delete event */
+			ipmr_sync_entry_delete(origin, group);
 		}
 	}
 
--- a/net/ipv6/ip6mr.c
+++ b/net/ipv6/ip6mr.c
@@ -69,6 +69,9 @@ static DEFINE_RWLOCK(mrt_lock);
 /* Special spinlock for queue of unresolved entries */
 static DEFINE_SPINLOCK(mfc_unres_lock);
 
+/* Spinlock for offload */
+static DEFINE_SPINLOCK(lock);
+
 /* We return to original Alan's scheme. Hash table of resolved
    entries is changed only in process context and protected
    with weak lock mrt_lock. Queue of unresolved entries is protected
@@ -94,6 +97,11 @@ static int ip6mr_rtm_dumproute(struct sk
 			       struct netlink_callback *cb);
 static void mroute_clean_tables(struct mr_table *mrt, int flags);
 static void ipmr_expire_process(struct timer_list *t);
+static struct mfc6_cache *ip6mr_cache_find(struct mr_table *mrt,
+					   const struct in6_addr *origin,
+					   const struct in6_addr *mcastgrp);
+static ip6mr_mfc_event_offload_callback_t __rcu
+				ip6mr_mfc_event_offload_callback;
 
 #ifdef CONFIG_IPV6_MROUTE_MULTIPLE_TABLES
 #define ip6mr_for_each_table(mrt, net) \
@@ -375,6 +383,82 @@ static struct mfc6_cache_cmp_arg ip6mr_m
 	.mf6c_mcastgrp = IN6ADDR_ANY_INIT,
 };
 
+/* ip6mr_sync_entry_update()
+ * Call the registered offload callback to report an update to a multicast
+ * route entry. The callback receives the list of destination interfaces and
+ * the interface count
+ */
+static void ip6mr_sync_entry_update(struct mr_table *mrt,
+				    struct mfc6_cache *cache)
+{
+	int vifi, dest_if_count = 0;
+	u32 dest_dev[MAXMIFS];
+	struct in6_addr mc_origin, mc_group;
+	ip6mr_mfc_event_offload_callback_t offload_update_cb_f;
+
+	memset(dest_dev, 0, sizeof(dest_dev));
+
+	spin_lock(&mrt_lock);
+
+	for (vifi = 0; vifi < cache->_c.mfc_un.res.maxvif; vifi++) {
+		if (!((cache->_c.mfc_un.res.ttls[vifi] > 0) &&
+		      (cache->_c.mfc_un.res.ttls[vifi] < 255))) {
+			continue;
+		}
+
+		if (dest_if_count == MAXMIFS) {
+			spin_unlock(&mrt_lock);
+			return;
+		}
+
+		if (!VIF_EXISTS(mrt, vifi)) {
+			spin_unlock(&mrt_lock);
+			return;
+		}
+
+		dest_dev[dest_if_count] = mrt->vif_table[vifi].dev->ifindex;
+		dest_if_count++;
+	}
+
+	memcpy(&mc_origin, &cache->mf6c_origin, sizeof(struct in6_addr));
+	memcpy(&mc_group, &cache->mf6c_mcastgrp, sizeof(struct in6_addr));
+	spin_unlock(&mrt_lock);
+
+	rcu_read_lock();
+	offload_update_cb_f = rcu_dereference(ip6mr_mfc_event_offload_callback);
+
+	if (!offload_update_cb_f) {
+		rcu_read_unlock();
+		return;
+	}
+
+	offload_update_cb_f(&mc_group, &mc_origin, dest_if_count, dest_dev,
+			    IP6MR_MFC_EVENT_UPDATE);
+	rcu_read_unlock();
+}
+
+/* ip6mr_sync_entry_delete()
+ * Call the registered offload callback to inform of a multicast route entry
+ * delete event
+ */
+static void ip6mr_sync_entry_delete(struct in6_addr *mc_origin,
+				    struct in6_addr *mc_group)
+{
+	ip6mr_mfc_event_offload_callback_t offload_update_cb_f;
+
+	rcu_read_lock();
+	offload_update_cb_f = rcu_dereference(ip6mr_mfc_event_offload_callback);
+
+	if (!offload_update_cb_f) {
+		rcu_read_unlock();
+		return;
+	}
+
+	offload_update_cb_f(mc_group, mc_origin, 0, NULL,
+			    IP6MR_MFC_EVENT_DELETE);
+	rcu_read_unlock();
+}
+
 static struct mr_table_ops ip6mr_mr_table_ops = {
 	.rht_params = &ip6mr_rht_params,
 	.cmparg_any = &ip6mr_mr_table_ops_cmparg_any,
@@ -694,6 +778,149 @@ static int call_ip6mr_mfc_entry_notifier
 				     &mfc->_c, tb_id, &net->ipv6.ipmr_seq);
 }
 
+/* ip6mr_register_mfc_event_offload_callback()
+ * Register the IPv6 multicast update callback for offload modules
+ */
+bool ip6mr_register_mfc_event_offload_callback(
+		ip6mr_mfc_event_offload_callback_t mfc_offload_cb)
+{
+	ip6mr_mfc_event_offload_callback_t offload_update_cb_f;
+
+	rcu_read_lock();
+	offload_update_cb_f = rcu_dereference(ip6mr_mfc_event_offload_callback);
+
+	if (offload_update_cb_f) {
+		rcu_read_unlock();
+		return false;
+	}
+	rcu_read_unlock();
+
+	spin_lock(&lock);
+	rcu_assign_pointer(ip6mr_mfc_event_offload_callback, mfc_offload_cb);
+	spin_unlock(&lock);
+	synchronize_rcu();
+	return true;
+}
+EXPORT_SYMBOL(ip6mr_register_mfc_event_offload_callback);
+
+/* ip6mr_unregister_mfc_event_offload_callback()
+ * De-register the IPv6 multicast update callback for offload modules
+ */
+void ip6mr_unregister_mfc_event_offload_callback(void)
+{
+	spin_lock(&lock);
+	rcu_assign_pointer(ip6mr_mfc_event_offload_callback, NULL);
+	spin_unlock(&lock);
+	synchronize_rcu();
+}
+EXPORT_SYMBOL(ip6mr_unregister_mfc_event_offload_callback);
+
+/* ip6mr_find_mfc_entry()
+ * Return the destination interface list for a particular multicast flow, and
+ * the number of interfaces in the list
+ */
+int ip6mr_find_mfc_entry(struct net *net, struct in6_addr *origin,
+			 struct in6_addr *group, u32 max_dest_cnt,
+			 u32 dest_dev[])
+{
+	int vifi, dest_if_count = 0;
+	struct mr_table *mrt;
+	struct mfc6_cache *cache;
+
+	mrt = ip6mr_get_table(net, RT6_TABLE_DFLT);
+	if (!mrt)
+		return -ENOENT;
+
+	spin_lock(&mrt_lock);
+	cache = ip6mr_cache_find(mrt, origin, group);
+	if (!cache) {
+		spin_unlock(&mrt_lock);
+		return -ENOENT;
+	}
+
+	for (vifi = 0; vifi < cache->_c.mfc_un.res.maxvif; vifi++) {
+		if (!((cache->_c.mfc_un.res.ttls[vifi] > 0) &&
+		      (cache->_c.mfc_un.res.ttls[vifi] < 255))) {
+			continue;
+		}
+
+		/* We have another valid destination interface entry. Check if
+		 * the number of the destination interfaces for the route is
+		 * exceeding the size of the array given to us
+		 */
+		if (dest_if_count == max_dest_cnt) {
+			spin_unlock(&mrt_lock);
+			return -EINVAL;
+		}
+
+		if (!VIF_EXISTS(mrt, vifi)) {
+			spin_unlock(&mrt_lock);
+			return -EINVAL;
+		}
+
+		dest_dev[dest_if_count] = mrt->vif_table[vifi].dev->ifindex;
+		dest_if_count++;
+	}
+	spin_unlock(&mrt_lock);
+
+	return dest_if_count;
+}
+EXPORT_SYMBOL(ip6mr_find_mfc_entry);
+
+/* ip6mr_mfc_stats_update()
+ * Update the MFC/VIF statistics for offloaded flows
+ */
+int ip6mr_mfc_stats_update(struct net *net, struct in6_addr *origin,
+			   struct in6_addr *group, u64 pkts_in,
+			   u64 bytes_in, uint64_t pkts_out,
+			   u64 bytes_out)
+{
+	int vif, vifi;
+	struct mr_table *mrt;
+	struct mfc6_cache *cache;
+
+	mrt = ip6mr_get_table(net, RT6_TABLE_DFLT);
+
+	if (!mrt)
+		return -ENOENT;
+
+	spin_lock(&mrt_lock);
+	cache = ip6mr_cache_find(mrt, origin, group);
+	if (!cache) {
+		spin_unlock(&mrt_lock);
+		return -ENOENT;
+	}
+
+	vif = cache->_c.mfc_parent;
+
+	if (!VIF_EXISTS(mrt, vif)) {
+		spin_unlock(&mrt_lock);
+		return -EINVAL;
+	}
+
+	mrt->vif_table[vif].pkt_in += pkts_in;
+	mrt->vif_table[vif].bytes_in += bytes_in;
+	cache->_c.mfc_un.res.pkt += pkts_out;
+	cache->_c.mfc_un.res.bytes += bytes_out;
+
+	for (vifi = cache->_c.mfc_un.res.minvif;
+			vifi < cache->_c.mfc_un.res.maxvif; vifi++) {
+		if ((cache->_c.mfc_un.res.ttls[vifi] > 0) &&
+		    (cache->_c.mfc_un.res.ttls[vifi] < 255)) {
+			if (!VIF_EXISTS(mrt, vifi)) {
+				spin_unlock(&mrt_lock);
+				return -EINVAL;
+			}
+			mrt->vif_table[vifi].pkt_out += pkts_out;
+			mrt->vif_table[vifi].bytes_out += bytes_out;
+		}
+	}
+
+	spin_unlock(&mrt_lock);
+	return 0;
+}
+EXPORT_SYMBOL(ip6mr_mfc_stats_update);
+
 /* Delete a VIF entry */
 static int mif6_delete(struct mr_table *mrt, int vifi, int notify,
 		       struct list_head *head)
@@ -1215,6 +1442,7 @@ static int ip6mr_mfc_delete(struct mr_ta
 			    int parent)
 {
 	struct mfc6_cache *c;
+	struct in6_addr mc_origin, mc_group;
 
 	/* The entries are added/deleted only under RTNL */
 	rcu_read_lock();
@@ -1223,6 +1451,9 @@ static int ip6mr_mfc_delete(struct mr_ta
 	rcu_read_unlock();
 	if (!c)
 		return -ENOENT;
+
+	memcpy(&mc_origin, &c->mf6c_origin, sizeof(struct in6_addr));
+	memcpy(&mc_group, &c->mf6c_mcastgrp, sizeof(struct in6_addr));
 	rhltable_remove(&mrt->mfc_hash, &c->_c.mnode, ip6mr_rht_params);
 	list_del_rcu(&c->_c.list);
 
@@ -1230,6 +1461,9 @@ static int ip6mr_mfc_delete(struct mr_ta
 				       FIB_EVENT_ENTRY_DEL, c, mrt->id);
 	mr6_netlink_event(mrt, c, RTM_DELROUTE);
 	mr_cache_put(&c->_c);
+	/* Inform offload modules of the delete event */
+	ip6mr_sync_entry_delete(&mc_origin, &mc_group);
+
 	return 0;
 }
 
@@ -1439,6 +1673,9 @@ static int ip6mr_mfc_add(struct net *net
 		call_ip6mr_mfc_entry_notifiers(net, FIB_EVENT_ENTRY_REPLACE,
 					       c, mrt->id);
 		mr6_netlink_event(mrt, c, RTM_NEWROUTE);
+
+		/* Inform offload modules of the update event */
+		ip6mr_sync_entry_update(mrt, c);
 		return 0;
 	}
 
@@ -1501,7 +1738,9 @@ static int ip6mr_mfc_add(struct net *net
 
 static void mroute_clean_tables(struct mr_table *mrt, int flags)
 {
+	struct mfc6_cache *cache;
 	struct mr_mfc *c, *tmp;
+	struct in6_addr mc_origin, mc_group;
 	LIST_HEAD(list);
 	int i;
 
@@ -1523,13 +1762,18 @@ static void mroute_clean_tables(struct m
 			if (((c->mfc_flags & MFC_STATIC) && !(flags & MRT6_FLUSH_MFC_STATIC)) ||
 			    (!(c->mfc_flags & MFC_STATIC) && !(flags & MRT6_FLUSH_MFC)))
 				continue;
+			cache = (struct mfc6_cache *)c;
+			memcpy(&mc_origin, &cache->mf6c_origin, sizeof(struct in6_addr));
+			memcpy(&mc_group, &cache->mf6c_mcastgrp, sizeof(struct in6_addr));
 			rhltable_remove(&mrt->mfc_hash, &c->mnode, ip6mr_rht_params);
 			list_del_rcu(&c->list);
 			call_ip6mr_mfc_entry_notifiers(read_pnet(&mrt->net),
 						       FIB_EVENT_ENTRY_DEL,
-						       (struct mfc6_cache *)c, mrt->id);
-			mr6_netlink_event(mrt, (struct mfc6_cache *)c, RTM_DELROUTE);
+						       cache, mrt->id);
+			mr6_netlink_event(mrt, cache, RTM_DELROUTE);
 			mr_cache_put(c);
+			/* Inform offload modules of the delete event */
+			ip6mr_sync_entry_delete(&mc_origin, &mc_group);
 		}
 	}
 
--- a/net/ipv6/route.c
+++ b/net/ipv6/route.c
@@ -192,6 +192,9 @@ static void rt6_uncached_list_flush_dev(
 	}
 }
 
+/* Define route change notification chain. */
+ATOMIC_NOTIFIER_HEAD(ip6route_chain);
+
 static inline const void *choose_neigh_daddr(const struct in6_addr *p,
 					     struct sk_buff *skb,
 					     const void *daddr)
@@ -4512,6 +4515,10 @@ int ipv6_route_ioctl(struct net *net, un
 		break;
 	}
 	rtnl_unlock();
+	if (!err)
+		atomic_notifier_call_chain(&ip6route_chain,
+					   SIOCADDRT ? RTM_NEWROUTE:RTM_DELROUTE, &cfg);
+
 	return err;
 }
 
@@ -5522,11 +5529,17 @@ static int inet6_rtm_delroute(struct sk_
 	}
 
 	if (cfg.fc_mp)
-		return ip6_route_multipath_del(&cfg, extack);
+		err = ip6_route_multipath_del(&cfg, extack);
 	else {
 		cfg.fc_delete_all_nh = 1;
-		return ip6_route_del(&cfg, extack);
+		err = ip6_route_del(&cfg, extack);
 	}
+
+	if (!err)
+		atomic_notifier_call_chain(&ip6route_chain,
+					   RTM_DELROUTE, &cfg);
+
+	return err;
 }
 
 static int inet6_rtm_newroute(struct sk_buff *skb, struct nlmsghdr *nlh,
@@ -5543,9 +5556,15 @@ static int inet6_rtm_newroute(struct sk_
 		cfg.fc_metric = IP6_RT_PRIO_USER;
 
 	if (cfg.fc_mp)
-		return ip6_route_multipath_add(&cfg, extack);
+		err = ip6_route_multipath_add(&cfg, extack);
 	else
-		return ip6_route_add(&cfg, GFP_KERNEL, extack);
+		err = ip6_route_add(&cfg, GFP_KERNEL, extack);
+
+	if (!err)
+		atomic_notifier_call_chain(&ip6route_chain,
+					   RTM_NEWROUTE, &cfg);
+
+	return err;
 }
 
 /* add the overhead of this fib6_nh to nexthop_len */
@@ -6343,6 +6362,18 @@ static int ip6_route_dev_notify(struct n
 	return NOTIFY_OK;
 }
 
+int rt6_register_notifier(struct notifier_block *nb)
+{
+	return atomic_notifier_chain_register(&ip6route_chain, nb);
+}
+EXPORT_SYMBOL(rt6_register_notifier);
+
+int rt6_unregister_notifier(struct notifier_block *nb)
+{
+	return atomic_notifier_chain_unregister(&ip6route_chain, nb);
+}
+EXPORT_SYMBOL(rt6_unregister_notifier);
+
 /*
  *	/proc
  */
--- a/net/l2tp/l2tp_ppp.c
+++ b/net/l2tp/l2tp_ppp.c
@@ -92,6 +92,7 @@
 #include <net/ip.h>
 #include <net/udp.h>
 #include <net/inet_common.h>
+#include <linux/if_pppox.h>
 
 #include <asm/byteorder.h>
 #include <linux/atomic.h>
@@ -123,9 +124,16 @@ struct pppol2tp_session {
 };
 
 static int pppol2tp_xmit(struct ppp_channel *chan, struct sk_buff *skb);
-
-static const struct ppp_channel_ops pppol2tp_chan_ops = {
-	.start_xmit =  pppol2tp_xmit,
+static int pppol2tp_get_channel_protocol(struct ppp_channel *);
+static int pppol2tp_get_channel_protocol_ver(struct ppp_channel *);
+static void pppol2tp_hold_chan(struct ppp_channel *);
+static void pppol2tp_release_chan(struct ppp_channel *);
+static const struct pppol2tp_channel_ops pppol2tp_chan_ops = {
+	.ops.start_xmit =  pppol2tp_xmit,
+	.ops.get_channel_protocol = pppol2tp_get_channel_protocol,
+	.ops.get_channel_protocol_ver = pppol2tp_get_channel_protocol_ver,
+	.ops.hold = pppol2tp_hold_chan,
+	.ops.release = pppol2tp_release_chan,
 };
 
 static const struct proto_ops pppol2tp_ops;
@@ -238,6 +246,7 @@ static void pppol2tp_recv(struct l2tp_se
 		struct pppox_sock *po;
 
 		po = pppox_sk(sk);
+		skb->skb_iif = ppp_dev_index(&po->chan);
 		ppp_input(&po->chan, skb);
 	} else {
 		if (sock_queue_rcv_skb(sk, skb) < 0) {
@@ -329,6 +338,104 @@ error:
 	return error;
 }
 
+/* pppol2tp_hold_chan() */
+static void pppol2tp_hold_chan(struct ppp_channel *chan)
+{
+	struct sock *sk = (struct sock *)chan->private;
+
+	sock_hold(sk);
+}
+
+/* pppol2tp_release_chan() */
+static void pppol2tp_release_chan(struct ppp_channel *chan)
+{
+	struct sock *sk = (struct sock *)chan->private;
+
+	sock_put(sk);
+}
+
+/* pppol2tp_get_channel_protocol()
+ * Return the protocol type of the L2TP over PPP protocol
+ */
+static int pppol2tp_get_channel_protocol(struct ppp_channel *chan)
+{
+	return PX_PROTO_OL2TP;
+}
+
+/* pppol2tp_get_channel_protocol_ver()
+ * Return the protocol version of the L2TP over PPP protocol
+ */
+static int pppol2tp_get_channel_protocol_ver(struct ppp_channel *chan)
+{
+	struct sock *sk;
+	struct l2tp_session *session;
+	struct l2tp_tunnel *tunnel;
+	int version = 0;
+
+	if (!(chan && chan->private)) {
+		return -1;
+	}
+
+	sk = (struct sock *)chan->private;
+
+	/* Get session and tunnel contexts from the socket */
+	session = pppol2tp_sock_to_session(sk);
+	if (!session) {
+		return -1;
+	}
+
+	tunnel = session->tunnel;
+	if (!tunnel) {
+		sock_put(sk);
+		return -1;
+	}
+
+	version = tunnel->version;
+
+	sock_put(sk);
+	return version;
+}
+
+/* pppol2tp_get_addressing() */
+static int pppol2tp_get_addressing(struct ppp_channel *chan,
+				   struct pppol2tp_common_addr *addr)
+{
+	struct sock *sk = (struct sock *)chan->private;
+	struct l2tp_session *session;
+	struct l2tp_tunnel *tunnel;
+	struct inet_sock *isk = NULL;
+	int err = -ENXIO;
+
+	/* Get session and tunnel contexts from the socket */
+	session = pppol2tp_sock_to_session(sk);
+	if (!session)
+		return err;
+
+	tunnel = session->tunnel;
+	isk = inet_sk(tunnel->sock);
+
+	addr->local_tunnel_id = tunnel->tunnel_id;
+	addr->remote_tunnel_id = tunnel->peer_tunnel_id;
+	addr->local_session_id = session->session_id;
+	addr->remote_session_id = session->peer_session_id;
+
+	addr->local_addr.sin_port = isk->inet_sport;
+	addr->remote_addr.sin_port = isk->inet_dport;
+	addr->local_addr.sin_addr.s_addr = isk->inet_saddr;
+	addr->remote_addr.sin_addr.s_addr = isk->inet_daddr;
+
+	sock_put(sk);
+	return 0;
+}
+
+/* pppol2tp_channel_addressing_get() */
+int pppol2tp_channel_addressing_get(struct ppp_channel *chan,
+				    struct pppol2tp_common_addr *addr)
+{
+	return pppol2tp_get_addressing(chan, addr);
+}
+EXPORT_SYMBOL(pppol2tp_channel_addressing_get);
+
 /* Transmit function called by generic PPP driver.  Sends PPP frame
  * over PPPoL2TP socket.
  *
@@ -373,6 +480,10 @@ static int pppol2tp_xmit(struct ppp_chan
 	__skb_push(skb, 2);
 	skb->data[0] = PPP_ALLSTATIONS;
 	skb->data[1] = PPP_UI;
+	/* set incoming interface as the ppp interface */
+	if ((skb->protocol == htons(ETH_P_IP)) ||
+	    (skb->protocol == htons(ETH_P_IPV6)))
+		skb->skb_iif = ppp_dev_index(chan);
 
 	local_bh_disable();
 	l2tp_xmit_skb(session, skb);
@@ -819,7 +930,7 @@ static int pppol2tp_connect(struct socke
 	po->chan.hdrlen = PPPOL2TP_L2TP_HDR_SIZE_NOSEQ;
 
 	po->chan.private = sk;
-	po->chan.ops	 = &pppol2tp_chan_ops;
+	po->chan.ops	 = &pppol2tp_chan_ops.ops;
 	po->chan.mtu	 = pppol2tp_tunnel_mtu(tunnel);
 
 	error = ppp_register_net_channel(sock_net(sk), &po->chan);
--- a/net/netfilter/Kconfig
+++ b/net/netfilter/Kconfig
@@ -167,6 +167,14 @@ config NF_CONNTRACK_DSCPREMARK_EXT
 	  This option enables support for connection tracking extension
 	  for dscp remark.
 
+config NF_CONNTRACK_CHAIN_EVENTS
+	bool "Register multiple callbacks to ct events"
+	depends on NF_CONNTRACK_EVENTS
+	help
+	  Support multiple registrations.
+
+	  If unsure, say `N'.
+
 config NF_CONNTRACK_TIMESTAMP
 	bool  'Connection tracking timestamping'
 	depends on NETFILTER_ADVANCED
--- a/net/netfilter/nf_conntrack_core.c
+++ b/net/netfilter/nf_conntrack_core.c
@@ -2909,6 +2909,9 @@ int nf_conntrack_init_net(struct net *ne
 	nf_conntrack_ecache_pernet_init(net);
 	nf_conntrack_proto_pernet_init(net);
 
+#ifdef CONFIG_NF_CONNTRACK_CHAIN_EVENTS
+	ATOMIC_INIT_NOTIFIER_HEAD(&net->ct.nf_conntrack_chain);
+#endif
 	return 0;
 
 err_expect:
--- a/net/netfilter/nf_conntrack_ecache.c
+++ b/net/netfilter/nf_conntrack_ecache.c
@@ -17,6 +17,9 @@
 #include <linux/vmalloc.h>
 #include <linux/stddef.h>
 #include <linux/err.h>
+#ifdef CONFIG_NF_CONNTRACK_CHAIN_EVENTS
+#include <linux/notifier.h>
+#endif
 #include <linux/kernel.h>
 #include <linux/netdevice.h>
 #include <linux/slab.h>
@@ -130,6 +133,7 @@ static void ecache_work(struct work_stru
 		schedule_delayed_work(&cnet->ecache.dwork, delay);
 }
 
+#ifndef CONFIG_NF_CONNTRACK_CHAIN_EVENTS
 static int __nf_conntrack_eventmask_report(struct nf_conntrack_ecache *e,
 					   const u32 events,
 					   const u32 missed,
@@ -204,9 +208,56 @@ int nf_conntrack_eventmask_report(unsign
 	return ret;
 }
 EXPORT_SYMBOL_GPL(nf_conntrack_eventmask_report);
+#endif
 
 /* deliver cached events and clear cache entry - must be called with locally
  * disabled softirqs */
+#ifdef CONFIG_NF_CONNTRACK_CHAIN_EVENTS
+void nf_ct_deliver_cached_events(struct nf_conn *ct)
+{
+	unsigned long events, missed;
+	struct nf_conntrack_ecache *e;
+	struct nf_ct_event item;
+	struct net *net = nf_ct_net(ct);
+	int ret = 0;
+
+	e = nf_ct_ecache_find(ct);
+	if (!e)
+		return;
+
+	events = xchg(&e->cache, 0);
+
+	if (!nf_ct_is_confirmed(ct) || nf_ct_is_dying(ct) || !events)
+		return;
+
+	/*
+	 * We make a copy of the missed event cache without taking
+	 * the lock, thus we may send missed events twice. However,
+	 * this does not harm and it happens very rarely.
+	 */
+	missed = e->missed;
+
+	if (!((events | missed) & e->ctmask))
+		return;
+
+	item.ct = ct;
+	item.portid = 0;
+	item.report = 0;
+
+	atomic_notifier_call_chain(&net->ct.nf_conntrack_chain,
+				   events | missed, &item);
+
+	if (likely(ret >= 0 && !missed))
+		return;
+
+	spin_lock_bh(&ct->lock);
+	if (ret < 0)
+		e->missed |= events;
+	else
+		e->missed &= ~missed;
+	spin_unlock_bh(&ct->lock);
+}
+#else
 void nf_ct_deliver_cached_events(struct nf_conn *ct)
 {
 	struct nf_conntrack_ecache *e;
@@ -232,6 +283,7 @@ void nf_ct_deliver_cached_events(struct
 	 */
 	__nf_conntrack_eventmask_report(e, events, e->missed, &item);
 }
+#endif
 EXPORT_SYMBOL_GPL(nf_ct_deliver_cached_events);
 
 void nf_ct_expect_event_report(enum ip_conntrack_expect_events event,
@@ -264,6 +316,12 @@ out_unlock:
 	rcu_read_unlock();
 }
 
+#ifdef CONFIG_NF_CONNTRACK_CHAIN_EVENTS
+int nf_conntrack_register_notifier(struct net *net, struct notifier_block *nb)
+{
+	return atomic_notifier_chain_register(&net->ct.nf_conntrack_chain, nb);
+}
+#else
 void nf_conntrack_register_notifier(struct net *net,
 				    const struct nf_ct_event_notifier *new)
 {
@@ -276,8 +334,16 @@ void nf_conntrack_register_notifier(stru
 	rcu_assign_pointer(net->ct.nf_conntrack_event_cb, new);
 	mutex_unlock(&nf_ct_ecache_mutex);
 }
+#endif
 EXPORT_SYMBOL_GPL(nf_conntrack_register_notifier);
 
+#ifdef CONFIG_NF_CONNTRACK_CHAIN_EVENTS
+int nf_conntrack_unregister_notifier(struct net *net, struct notifier_block *nb)
+{
+	return atomic_notifier_chain_unregister(&net->ct.nf_conntrack_chain,
+						nb);
+}
+#else
 void nf_conntrack_unregister_notifier(struct net *net)
 {
 	mutex_lock(&nf_ct_ecache_mutex);
@@ -285,6 +351,7 @@ void nf_conntrack_unregister_notifier(st
 	mutex_unlock(&nf_ct_ecache_mutex);
 	/* synchronize_rcu() is called after netns pre_exit */
 }
+#endif
 EXPORT_SYMBOL_GPL(nf_conntrack_unregister_notifier);
 
 void nf_conntrack_ecache_work(struct net *net, enum nf_ct_ecache_state state)
--- a/net/netfilter/nf_conntrack_netlink.c
+++ b/net/netfilter/nf_conntrack_netlink.c
@@ -28,6 +28,9 @@
 #include <linux/netlink.h>
 #include <linux/spinlock.h>
 #include <linux/interrupt.h>
+#ifdef CONFIG_NF_CONNTRACK_CHAIN_EVENTS
+#include <linux/notifier.h>
+#endif
 #include <linux/slab.h>
 #include <linux/siphash.h>
 
@@ -711,18 +714,26 @@ static size_t ctnetlink_nlmsg_size(const
 	       ;
 }
 
+#ifdef CONFIG_NF_CONNTRACK_CHAIN_EVENTS
+static int ctnetlink_conntrack_event(struct notifier_block *this,
+				     unsigned long events, void *ptr)
+#else
 static int
 ctnetlink_conntrack_event(unsigned int events, const struct nf_ct_event *item)
+#endif
 {
 	const struct nf_conntrack_zone *zone;
 	struct net *net;
 	struct nlmsghdr *nlh;
 	struct nlattr *nest_parms;
-	struct nf_conn *ct = item->ct;
 	struct sk_buff *skb;
 	unsigned int type;
 	unsigned int flags = 0, group;
 	int err;
+#ifdef CONFIG_NF_CONNTRACK_CHAIN_EVENTS
+	struct nf_ct_event *item = (struct nf_ct_event *)ptr;
+#endif
+	struct nf_conn *ct = item->ct;
 
 	if (events & (1 << IPCT_DESTROY)) {
 		type = IPCTNL_MSG_CT_DELETE;
@@ -3112,6 +3123,7 @@ nla_put_failure:
 }
 
 #ifdef CONFIG_NF_CONNTRACK_EVENTS
+#ifndef CONFIG_NF_CONNTRACK_CHAIN_EVENTS
 static int
 ctnetlink_expect_event(unsigned int events, const struct nf_exp_event *item)
 {
@@ -3161,6 +3173,7 @@ errout:
 	return 0;
 }
 #endif
+#endif
 static int ctnetlink_exp_done(struct netlink_callback *cb)
 {
 	if (cb->args[1])
@@ -3765,11 +3778,17 @@ static int ctnetlink_stat_exp_cpu(struct
 }
 
 #ifdef CONFIG_NF_CONNTRACK_EVENTS
+#ifdef CONFIG_NF_CONNTRACK_CHAIN_EVENTS
+static struct notifier_block ctnl_notifier = {
+	.notifier_call = ctnetlink_conntrack_event,
+};
+#else
 static struct nf_ct_event_notifier ctnl_notifier = {
 	.ct_event = ctnetlink_conntrack_event,
 	.exp_event = ctnetlink_expect_event,
 };
 #endif
+#endif
 
 static const struct nfnl_callback ctnl_cb[IPCTNL_MSG_MAX] = {
 	[IPCTNL_MSG_CT_NEW]	= {
@@ -3868,8 +3887,12 @@ static int __net_init ctnetlink_net_init
 static void ctnetlink_net_pre_exit(struct net *net)
 {
 #ifdef CONFIG_NF_CONNTRACK_EVENTS
+#ifdef CONFIG_NF_CONNTRACK_CHAIN_EVENTS
+	nf_conntrack_unregister_notifier(net, &ctnl_notifier);
+#else
 	nf_conntrack_unregister_notifier(net);
 #endif
+#endif
 }
 
 static struct pernet_operations ctnetlink_net_ops = {
--- a/net/netfilter/nf_conntrack_proto_udp.c
+++ b/net/netfilter/nf_conntrack_proto_udp.c
@@ -29,10 +29,11 @@ static const unsigned int udp_timeouts[U
 	[UDP_CT_REPLIED]	= 120*HZ,
 };
 
-static unsigned int *udp_get_timeouts(struct net *net)
+unsigned int *udp_get_timeouts(struct net *net)
 {
 	return nf_udp_pernet(net)->timeouts;
 }
+EXPORT_SYMBOL(udp_get_timeouts);
 
 static void udp_error_log(const struct sk_buff *skb,
 			  const struct nf_hook_state *state,
--- a/tools/include/uapi/linux/in.h
+++ b/tools/include/uapi/linux/in.h
@@ -62,6 +62,8 @@ enum {
 #define IPPROTO_MTP		IPPROTO_MTP
   IPPROTO_BEETPH = 94,		/* IP option pseudo header for BEET	*/
 #define IPPROTO_BEETPH		IPPROTO_BEETPH
+  IPPROTO_ETHERIP = 97,		/* ETHERIP protocol number		*/
+#define IPPROTO_ETHERIP		IPPROTO_ETHERIP
   IPPROTO_ENCAP = 98,		/* Encapsulation Header			*/
 #define IPPROTO_ENCAP		IPPROTO_ENCAP
   IPPROTO_PIM = 103,		/* Protocol Independent Multicast	*/
@@ -330,7 +332,7 @@ struct sockaddr_in {
 #endif
 
 /* <asm/byteorder.h> contains the htonl type stuff.. */
-#include <asm/byteorder.h> 
+#include <asm/byteorder.h>
 
 
 #endif /* _UAPI_LINUX_IN_H */
