From 7ca3aefb0d38e78279882edf617fad47908f32b1 Mon Sep 17 00:00:00 2001
From: Chaithanya Garrepalli <cgarre@codeaurora.org>
Date: Mon, 14 Mar 2016 10:42:09 +0530
Subject: [PATCH] skbuff_debug: update sum after operation on recycle pool

skbuff next and prev of buffers in recycle list will be
while insert or delete from list, taken care of updating
sum after these operations

Change-Id: I97a7a51712d49ed0067c218fece0718a1d188b63
Signed-off-by: Chaithanya Garrepalli <cgarre@codeaurora.org>
Signed-off-by: Casey Chen <kexinc@codeaurora.org>
---
 net/core/skbuff_debug.c   | 27 ++++++++++++++++++---
 net/core/skbuff_debug.h   |  4 +++
 net/core/skbuff_recycle.c | 51 ++++++++++++++++++++++++++++++++-------
 3 files changed, 70 insertions(+), 12 deletions(-)

--- a/net/core/skbuff_debug.c
+++ b/net/core/skbuff_debug.c
@@ -140,10 +140,9 @@ inline void skbuff_debugobj_activate(str
 	if (ret)
 		goto err_act;
 
-	if (skb->sum == skbuff_debugobj_sum(skb))
-		return;
+	skbuff_debugobj_sum_validate(skb);
 
-	pr_emerg("skb_debug: skb changed while deactive\n");
+	return;
 
 err_act:
 	ftrace_dump(DUMP_ALL);
@@ -189,6 +188,28 @@ inline void skbuff_debugobj_deactivate(s
 	skbuff_debugobj_print_skb(skb);
 }
 
+inline void skbuff_debugobj_sum_validate(struct sk_buff *skb)
+{
+	if (!skbuff_debugobj_enabled || !skb)
+		return;
+
+	if (skb->sum == skbuff_debugobj_sum(skb))
+		return;
+
+	ftrace_dump(DUMP_ALL);
+	WARN(1, "skb_debug: skb changed while deactive skb = 0x%p sum = %d (%d)\n",
+	     skb, skb->sum, skbuff_debugobj_sum(skb));
+	skbuff_debugobj_print_skb(skb);
+}
+
+inline void skbuff_debugobj_sum_update(struct sk_buff *skb)
+{
+	if (!skbuff_debugobj_enabled || !skb)
+		return;
+
+	skb->sum = skbuff_debugobj_sum(skb);
+}
+
 inline void skbuff_debugobj_destroy(struct sk_buff *skb)
 {
 	if (!skbuff_debugobj_enabled)
--- a/net/core/skbuff_debug.h
+++ b/net/core/skbuff_debug.h
@@ -25,11 +25,15 @@ void skbuff_debugobj_init_and_activate(s
 void skbuff_debugobj_activate(struct sk_buff *skb);
 void skbuff_debugobj_deactivate(struct sk_buff *skb);
 void skbuff_debugobj_destroy(struct sk_buff *skb);
+void skbuff_debugobj_sum_validate(struct sk_buff *skb);
+void skbuff_debugobj_sum_update(struct sk_buff *skb);
 #else
 static inline void skbuff_debugobj_init_and_activate(struct sk_buff *skb) { }
 static inline void skbuff_debugobj_activate(struct sk_buff *skb) { }
 static inline void skbuff_debugobj_deactivate(struct sk_buff *skb) { }
 static inline void skbuff_debugobj_destroy(struct sk_buff *skb) { }
+static inline void skbuff_debugobj_sum_validate(struct sk_buff *skb) { }
+static inline void skbuff_debugobj_sum_update(struct sk_buff *skb) { }
 #endif
 
 #endif /* _LINUX_SKBBUFF_DEBUG_OBJECTS */
--- a/net/core/skbuff_recycle.c
+++ b/net/core/skbuff_recycle.c
@@ -37,6 +37,7 @@ inline struct sk_buff *skb_recycler_allo
 	unsigned long flags;
 	struct sk_buff_head *h;
 	struct sk_buff *skb = NULL;
+	struct sk_buff *ln = NULL;
 
 	if (unlikely(length > SKB_RECYCLE_SIZE))
 		return NULL;
@@ -45,8 +46,14 @@ inline struct sk_buff *skb_recycler_allo
 	local_irq_save(flags);
 	skb = skb_peek(h);
 	if (skb) {
+		ln = skb_peek_next(skb, h);
 		skbuff_debugobj_activate(skb);
+		/* Recalculate the sum for skb->next as next and prev pointers
+		 * of skb->next will be updated in __skb_unlink
+		 */
+		skbuff_debugobj_sum_validate(ln);
 		__skb_unlink(skb, h);
+		skbuff_debugobj_sum_update(ln);
 	}
 #ifdef CONFIG_SKB_RECYCLER_MULTI_CPU
 	if (unlikely(!skb)) {
@@ -62,11 +69,11 @@ inline struct sk_buff *skb_recycler_allo
 			struct sk_buff *gp = glob_recycler.pool[head].prev;
 
 			/* Move SKBs from global list to CPU pool */
-			skbuff_debugobj_activate(gn);
-			skbuff_debugobj_activate(gp);
+			skbuff_debugobj_sum_validate(gn);
+			skbuff_debugobj_sum_validate(gp);
 			skb_queue_splice_init(&glob_recycler.pool[head], h);
-			skbuff_debugobj_deactivate(gn);
-			skbuff_debugobj_deactivate(gp);
+			skbuff_debugobj_sum_update(gn);
+			skbuff_debugobj_sum_update(gp);
 
 			head = (head + 1) & SKB_RECYCLE_MAX_SHARED_POOLS_MASK;
 			glob_recycler.head = head;
@@ -74,8 +81,15 @@ inline struct sk_buff *skb_recycler_allo
 			/* We have refilled the CPU pool - dequeue */
 			skb = skb_peek(h);
 			if (skb) {
+				/* Recalculate the sum for skb->next as next and
+				 * prev pointers of skb->next will be updated
+				 * in __skb_unlink
+				 */
+				ln = skb_peek_next(skb, h);
 				skbuff_debugobj_activate(skb);
+				skbuff_debugobj_sum_validate(ln);
 				__skb_unlink(skb, h);
+				skbuff_debugobj_sum_update(ln);
 			}
 		}
 	}
@@ -114,7 +128,7 @@ inline bool skb_recycler_consume(struct
 {
 	unsigned long flags;
 	struct sk_buff_head *h;
-
+	struct sk_buff *ln = NULL;
 	/* Can we recycle this skb?  If not, simply return that we cannot */
 	if (unlikely(!consume_skb_can_recycle(skb, SKB_RECYCLE_MIN_SIZE,
 					      SKB_RECYCLE_MAX_SIZE)))
@@ -127,8 +141,14 @@ inline bool skb_recycler_consume(struct
 	local_irq_save(flags);
 	/* Attempt to enqueue the CPU hot recycle list first */
 	if (likely(skb_queue_len(h) < skb_recycle_max_skbs)) {
+		ln = skb_peek(h);
+		/* Recalculate the sum for peek of list as next and prev
+		 * pointers of skb->next will be updated in __skb_queue_head
+		 */
+		skbuff_debugobj_sum_validate(ln);
 		__skb_queue_head(h, skb);
 		skbuff_debugobj_deactivate(skb);
+		skbuff_debugobj_sum_update(ln);
 		local_irq_restore(flags);
 		preempt_enable();
 		return true;
@@ -151,20 +171,27 @@ inline bool skb_recycler_consume(struct
 			struct sk_buff *hn = h->next, *hp = h->prev;
 
 			/* Move SKBs from CPU pool to Global pool*/
-			skbuff_debugobj_activate(hp);
-			skbuff_debugobj_activate(hn);
+			skbuff_debugobj_sum_validate(hp);
+			skbuff_debugobj_sum_validate(hn);
 			skb_queue_splice_init(h, p);
-			skbuff_debugobj_deactivate(hp);
-			skbuff_debugobj_deactivate(hn);
+			skbuff_debugobj_sum_update(hp);
+			skbuff_debugobj_sum_update(hn);
 
 			/* Done with global list init */
 			glob_recycler.tail = next_tail;
 			spin_unlock(&glob_recycler.lock);
 
+			/* Recalculate the sum for peek of list as next and prev
+			 * pointers of skb->next will be updated in
+			 * __skb_queue_head
+			 */
+			ln = skb_peek(h);
+			skbuff_debugobj_sum_validate(ln);
 			/* We have now cleared room in the spare;
 			 * Initialize and enqueue skb into spare
 			 */
 			__skb_queue_head(h, skb);
+			skbuff_debugobj_sum_update(ln);
 			skbuff_debugobj_deactivate(skb);
 
 			local_irq_restore(flags);
@@ -175,8 +202,14 @@ inline bool skb_recycler_consume(struct
 		spin_unlock(&glob_recycler.lock);
 	} else {
 		/* We have room in the spare list; enqueue to spare list */
+		ln = skb_peek(h);
+		/* Recalculate the sum for peek of list as next and prev
+		 * pointers of skb->next will be updated in __skb_queue_head
+		 */
+		skbuff_debugobj_sum_validate(ln);
 		__skb_queue_head(h, skb);
 		skbuff_debugobj_deactivate(skb);
+		skbuff_debugobj_sum_update(ln);
 		local_irq_restore(flags);
 		preempt_enable();
 		return true;
