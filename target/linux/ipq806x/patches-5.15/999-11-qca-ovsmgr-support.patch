--- a/include/uapi/linux/openvswitch.h
+++ b/include/uapi/linux/openvswitch.h
@@ -274,6 +274,7 @@ enum ovs_vport_attr {
 	OVS_VPORT_ATTR_PAD,
 	OVS_VPORT_ATTR_IFINDEX,
 	OVS_VPORT_ATTR_NETNSID,
+	OVS_VPORT_ATTR_MASTER,	/* Master device for this vport, device name */
 	__OVS_VPORT_ATTR_MAX
 };
 
--- a/net/openvswitch/datapath.c
+++ b/net/openvswitch/datapath.c
@@ -48,6 +48,8 @@
 #include "vport-netdev.h"
 
 unsigned int ovs_net_id __read_mostly;
+static struct ovs_accel_callback __rcu *ovs_accel_cb;
+static struct srcu_struct ovs_accel_cb_sp_rcu;
 
 static struct genl_family dp_packet_genl_family;
 static struct genl_family dp_flow_genl_family;
@@ -219,6 +221,124 @@ void ovs_dp_detach_port(struct vport *p)
 	ovs_vport_del(p);
 }
 
+/* Notify datapath add event to acceleration callback */
+static void ovs_dp_add_notify(struct datapath *dp, struct vport *vp)
+{
+	struct ovs_accel_callback *ovs_cb;
+	int idx = srcu_read_lock(&ovs_accel_cb_sp_rcu);
+	ovs_cb = rcu_dereference(ovs_accel_cb);
+	if (ovs_cb && ovs_cb->ovs_accel_dp_add)
+		ovs_cb->ovs_accel_dp_add((void *)dp, vp->dev);
+	srcu_read_unlock(&ovs_accel_cb_sp_rcu, idx);
+}
+
+/* Notify datapath delete event to acceleration callback */
+static void ovs_dp_del_notify(struct datapath *dp, struct vport *vp)
+{
+	struct ovs_accel_callback *ovs_cb;
+
+	int idx = srcu_read_lock(&ovs_accel_cb_sp_rcu);
+	ovs_cb = rcu_dereference(ovs_accel_cb);
+	if (ovs_cb && ovs_cb->ovs_accel_dp_del)
+		ovs_cb->ovs_accel_dp_del((void *)dp, vp->dev);
+	srcu_read_unlock(&ovs_accel_cb_sp_rcu, idx);
+}
+
+/* Notify datapath port add event to acceleration callback */
+static void ovs_dp_port_add_notify(struct datapath *dp, struct vport *vp,
+				   struct nlattr **a)
+{
+	struct ovs_accel_callback *ovs_cb;
+	const char *master = NULL;
+	int idx;
+
+	if (a[OVS_VPORT_ATTR_MASTER])
+		master = nla_data(a[OVS_VPORT_ATTR_MASTER]);
+
+	idx = srcu_read_lock(&ovs_accel_cb_sp_rcu);
+	ovs_cb = rcu_dereference(ovs_accel_cb);
+	if (ovs_cb && ovs_cb->ovs_accel_dp_port_add)
+		ovs_cb->ovs_accel_dp_port_add((void *)dp, (void *)vp,
+					      vp->port_no, vp->ops->type,
+					      master, vp->dev);
+	srcu_read_unlock(&ovs_accel_cb_sp_rcu, idx);
+}
+
+/* Notify datapath port delete event to acceleration callback */
+void ovs_dp_port_del_notify(struct datapath *dp, struct vport *vp)
+{
+	struct ovs_accel_callback *ovs_cb;
+
+	int idx = srcu_read_lock(&ovs_accel_cb_sp_rcu);
+	ovs_cb = rcu_dereference(ovs_accel_cb);
+	if (ovs_cb && ovs_cb->ovs_accel_dp_port_del)
+		ovs_cb->ovs_accel_dp_port_del((void *)dp, (void *)vp, vp->dev);
+	srcu_read_unlock(&ovs_accel_cb_sp_rcu, idx);
+}
+
+/* Notify datapath flow add event to acceleration callback */
+static void ovs_dp_flow_add_notify(struct datapath *dp, struct sw_flow *sf)
+{
+	struct ovs_accel_callback *ovs_cb;
+
+	int idx = srcu_read_lock(&ovs_accel_cb_sp_rcu);
+	ovs_cb = rcu_dereference(ovs_accel_cb);
+	if (ovs_cb && ovs_cb->ovs_accel_dp_flow_add)
+		ovs_cb->ovs_accel_dp_flow_add((void *)dp, sf);
+	srcu_read_unlock(&ovs_accel_cb_sp_rcu, idx);
+}
+
+/* Notify datapath flow delete event to acceleration callback */
+static void ovs_dp_flow_del_notify(struct datapath *dp, struct sw_flow *sf)
+{
+	struct ovs_accel_callback *ovs_cb;
+
+	int idx = srcu_read_lock(&ovs_accel_cb_sp_rcu);
+	ovs_cb = rcu_dereference(ovs_accel_cb);
+	if (ovs_cb && ovs_cb->ovs_accel_dp_flow_del)
+		ovs_cb->ovs_accel_dp_flow_del((void *)dp, sf);
+	srcu_read_unlock(&ovs_accel_cb_sp_rcu, idx);
+}
+
+/* Notify datapath flow table flush event to acceleration callback */
+static void ovs_dp_flow_tbl_flush_notify(struct datapath *dp)
+{
+	struct ovs_accel_callback *ovs_cb;
+
+	int idx = srcu_read_lock(&ovs_accel_cb_sp_rcu);
+	ovs_cb = rcu_dereference(ovs_accel_cb);
+	if (ovs_cb && ovs_cb->ovs_accel_dp_flow_tbl_flush)
+		ovs_cb->ovs_accel_dp_flow_tbl_flush((void *)dp);
+	srcu_read_unlock(&ovs_accel_cb_sp_rcu, idx);
+}
+
+/* Notify datapath flow set/change event to acceleration callback */
+static void ovs_dp_flow_set_notify(struct datapath *dp, struct sw_flow *sf,
+				   struct sw_flow_actions *new_sfa)
+{
+	struct ovs_accel_callback *ovs_cb;
+
+	int idx = srcu_read_lock(&ovs_accel_cb_sp_rcu);
+	ovs_cb = rcu_dereference(ovs_accel_cb);
+	if (ovs_cb && ovs_cb->ovs_accel_dp_flow_set)
+		ovs_cb->ovs_accel_dp_flow_set((void *)dp, sf, new_sfa);
+	srcu_read_unlock(&ovs_accel_cb_sp_rcu, idx);
+}
+
+/* Forward datapath packet to acceleration callback
+ * Must be called with rcu_read_lock.
+ */
+static void ovs_dp_pkt_process_notify(struct datapath *dp, struct sk_buff *skb,
+				      struct sw_flow_key *key, struct sw_flow *sf,
+		struct sw_flow_actions *sfa)
+{
+	struct ovs_accel_callback *ovs_cb;
+
+	ovs_cb = rcu_dereference(ovs_accel_cb);
+	if (ovs_cb && ovs_cb->ovs_accel_dp_pkt_process)
+		ovs_cb->ovs_accel_dp_pkt_process((void *)dp, skb, key, sf, sfa);
+}
+
 /* Must be called with rcu_read_lock. */
 void ovs_dp_process_packet(struct sk_buff *skb, struct sw_flow_key *key)
 {
@@ -234,6 +354,8 @@ void ovs_dp_process_packet(struct sk_buf
 
 	stats = this_cpu_ptr(dp->stats_percpu);
 
+	ovs_dp_pkt_process_notify(dp, skb, key, NULL, NULL);
+
 	/* Look up flow. */
 	flow = ovs_flow_tbl_lookup_stats(&dp->table, key, skb_get_hash(skb),
 					 &n_mask_hit, &n_cache_hit);
@@ -268,6 +390,7 @@ void ovs_dp_process_packet(struct sk_buf
 
 	ovs_flow_stats_update(flow, key->tp.flags, skb);
 	sf_acts = rcu_dereference(flow->sf_acts);
+	ovs_dp_pkt_process_notify(dp, skb, key, flow, sf_acts);
 	error = ovs_execute_actions(dp, skb, sf_acts, key);
 	if (unlikely(error))
 		net_dbg_ratelimited("ovs: action execution error on datapath %s: %d\n",
@@ -1031,6 +1154,7 @@ static int ovs_flow_cmd_new(struct sk_bu
 			goto err_unlock_ovs;
 		}
 
+		ovs_dp_flow_add_notify(dp, new_flow);
 		if (unlikely(reply)) {
 			error = ovs_flow_cmd_fill_info(new_flow,
 						       ovs_header->dp_ifindex,
@@ -1243,6 +1367,7 @@ static int ovs_flow_cmd_set(struct sk_bu
 	if (likely(acts)) {
 		old_acts = ovsl_dereference(flow->sf_acts);
 		rcu_assign_pointer(flow->sf_acts, acts);
+		ovs_dp_flow_set_notify(dp, flow, old_acts);
 
 		if (unlikely(reply)) {
 			error = ovs_flow_cmd_fill_info(flow,
@@ -1378,6 +1503,7 @@ static int ovs_flow_cmd_del(struct sk_bu
 	}
 
 	if (unlikely(!a[OVS_FLOW_ATTR_KEY] && !ufid_present)) {
+		ovs_dp_flow_tbl_flush_notify(dp);
 		err = ovs_flow_tbl_flush(&dp->table);
 		goto unlock;
 	}
@@ -1392,6 +1518,7 @@ static int ovs_flow_cmd_del(struct sk_bu
 	}
 
 	ovs_flow_tbl_remove(&dp->table, flow);
+	ovs_dp_flow_del_notify(dp, flow);
 	ovs_unlock();
 
 	reply = ovs_flow_cmd_alloc_info((const struct sw_flow_actions __force *) flow->sf_acts,
@@ -1826,6 +1953,7 @@ static int ovs_dp_cmd_new(struct sk_buff
 
 	ovs_net = net_generic(ovs_dp_get_net(dp), ovs_net_id);
 	list_add_tail_rcu(&dp->list_node, &ovs_net->dps);
+	ovs_dp_add_notify(dp, vport);
 
 	ovs_unlock();
 
@@ -1866,6 +1994,7 @@ static void __dp_destroy(struct datapath
 				ovs_dp_detach_port(vport);
 	}
 
+	ovs_dp_del_notify(dp, ovs_vport_ovsl(dp, OVSP_LOCAL));
 	list_del_rcu(&dp->list_node);
 
 	/* OVSP_LOCAL is datapath internal port. We need to make sure that
@@ -2264,6 +2393,7 @@ restart:
 		goto exit_unlock_free;
 	}
 
+	ovs_dp_port_add_notify(dp, vport, a);
 	err = ovs_vport_cmd_fill_info(vport, reply, genl_info_net(info),
 				      info->snd_portid, info->snd_seq, 0,
 				      OVS_VPORT_CMD_NEW, GFP_KERNEL);
@@ -2316,7 +2446,6 @@ static int ovs_vport_cmd_set(struct sk_b
 			goto exit_unlock_free;
 	}
 
-
 	if (a[OVS_VPORT_ATTR_UPCALL_PID]) {
 		struct nlattr *ids = a[OVS_VPORT_ATTR_UPCALL_PID];
 
@@ -2365,6 +2494,7 @@ static int ovs_vport_cmd_del(struct sk_b
 		goto exit_unlock_free;
 	}
 
+	ovs_dp_port_del_notify(vport->dp, vport);
 	err = ovs_vport_cmd_fill_info(vport, reply, genl_info_net(info),
 				      info->snd_portid, info->snd_seq, 0,
 				      OVS_VPORT_CMD_DEL, GFP_KERNEL);
@@ -2395,6 +2525,276 @@ exit_unlock_free:
 	return err;
 }
 
+/* Register OVS datapath accelerator */
+int ovs_register_accelerator(struct ovs_accel_callback *oac)
+{
+	ovs_lock();
+
+	if (unlikely(rcu_access_pointer(ovs_accel_cb))) {
+		ovs_unlock();
+		return -EEXIST;
+	}
+
+	rcu_assign_pointer(ovs_accel_cb, oac);
+	ovs_unlock();
+	synchronize_srcu(&ovs_accel_cb_sp_rcu);
+	return 0;
+}
+EXPORT_SYMBOL(ovs_register_accelerator);
+
+/* Unregister OVS datapath accelerator */
+void ovs_unregister_accelerator(struct ovs_accel_callback *oac)
+{
+	ovs_lock();
+	rcu_assign_pointer(ovs_accel_cb, NULL);
+	ovs_unlock();
+	synchronize_srcu(&ovs_accel_cb_sp_rcu);
+}
+EXPORT_SYMBOL(ovs_unregister_accelerator);
+
+/* Find datapath flow rule using the key*/
+struct sw_flow *ovs_accel_flow_find(void *dp_inst, struct sw_flow_key *key)
+{
+	struct datapath *dp = dp_inst;
+	struct sw_flow *flow;
+
+	rcu_read_lock();
+	flow = ovs_flow_tbl_lookup(&dp->table, key);
+	rcu_read_unlock();
+
+	return flow;
+}
+EXPORT_SYMBOL(ovs_accel_flow_find);
+
+/* Find datapath flow rule using MAC addresses*/
+struct sw_flow *ovs_accel_flow_find_by_mac(void *dp_inst,
+						struct net_device *indev,
+						struct net_device *outdev,
+						uint8_t *smac, uint8_t *dmac, uint16_t type)
+{
+	struct datapath *dp = dp_inst;
+	struct table_instance *ti;
+	struct sw_flow *flow = NULL;
+	struct sw_flow_actions *sf_acts;
+	const struct nlattr *a;
+	struct vport *vport;
+	bool flow_found = false;
+	int rem;
+	int i;
+
+	rcu_read_lock();
+	ti = rcu_dereference(dp->table.ti);
+
+	for (i = 0; i < ti->n_buckets; i++) {
+		struct hlist_head *head =  &ti->buckets[i];
+		struct hlist_node *n;
+
+		if (unlikely(!head))
+			continue;
+
+		hlist_for_each_entry_safe(flow, n, head,
+				flow_table.node[ti->node_ver]) {
+			if ((flow->key.eth.type == type) &&
+			     ether_addr_equal(flow->key.eth.src, smac) &&
+			     ether_addr_equal(flow->key.eth.dst, dmac)) {
+				flow_found = true;
+				goto found;
+			}
+		}
+	}
+found:
+	if (!flow_found) {
+		rcu_read_unlock();
+		return NULL;
+	}
+
+	/*
+	 * Flow is found, check if ingress port matches indev
+	 */
+	if (!indev) {
+		goto check_outdev;
+	}
+
+	vport = ovs_vport_ovsl_rcu(dp, flow->key.phy.in_port);
+	if (!vport || (indev != vport->dev)) {
+		rcu_read_unlock();
+		return NULL;
+	}
+
+check_outdev:
+	/*
+	 * if outdev is NULL, then the API is called
+	 * to find the flow only
+	 */
+	if (!outdev) {
+		rcu_read_unlock();
+		return flow;
+	}
+
+	/*
+	 * Flow is found, check if output action is outdev
+	 */
+	flow_found = false;
+	sf_acts = rcu_dereference(flow->sf_acts);
+	for (a = sf_acts->actions, rem = sf_acts->actions_len; rem > 0;
+			a = nla_next(a, &rem)) {
+		int port_no;
+
+		if (nla_type(a) != OVS_ACTION_ATTR_OUTPUT)
+			continue;
+
+		port_no = nla_get_u32(a);
+		vport = ovs_vport_ovsl_rcu(dp, port_no);
+
+		if (vport && (outdev == vport->dev)) {
+			flow_found = true;
+		}
+	}
+
+	if (!flow_found)
+		flow = NULL;
+
+	rcu_read_unlock();
+	return flow;
+}
+EXPORT_SYMBOL(ovs_accel_flow_find_by_mac);
+
+/* Update flow rule statistics */
+int ovs_accel_flow_stats_update(void *dp_inst, void *out_vport,
+				 struct sw_flow_key *key, int pkts, int bytes)
+{
+	struct datapath *dp = dp_inst;
+	struct sw_flow_stats *stats;
+	struct sw_flow *flow;
+	struct dp_stats_percpu *dp_stats;
+	int node = numa_node_id();
+	u64 *stats_counter;
+	u32 n_mask_hit;
+	u32 n_cache_hit;
+
+	rcu_read_lock();
+	flow = ovs_flow_tbl_lookup_stats(&dp->table, key, 0,
+					&n_mask_hit, &n_cache_hit);
+	if (!flow) {
+		rcu_read_unlock();
+		return -EINVAL;
+	}
+
+	/* Update node specific statistics, if memory is not allocated
+	 * for this node then update in 0 node
+	 */
+	stats = rcu_dereference(flow->stats[node]);
+	if (unlikely(!stats))
+		stats = rcu_dereference(flow->stats[0]);
+
+	rcu_read_unlock();
+
+	spin_lock(&stats->lock);
+	stats->used = jiffies;
+	stats->packet_count += pkts;
+	stats->byte_count += bytes;
+
+	/* Update datapath statistics, only hit count should be updated here,
+	 * miss count is taken care by datapath.
+	 * n_mask_hit and stats_counter are updated per packet, whereas
+	 * stats_counter will match the number of packets processed in datapath
+	 * n_mask_hit is updated number of packets times the total masks that
+	 * are processed.  Datapath flows are now accelerated and this API is
+	 * called to update flow statistics, datpath statistics should use
+	 * number of packets.
+	 */
+	dp_stats = this_cpu_ptr(dp->stats_percpu);
+	stats_counter = &dp_stats->n_hit;
+
+	u64_stats_update_begin(&dp_stats->syncp);
+	(*stats_counter) += pkts;
+	dp_stats->n_mask_hit += n_mask_hit * pkts;
+	dp_stats->n_cache_hit += n_cache_hit * pkts;
+	u64_stats_update_end(&dp_stats->syncp);
+
+	spin_unlock(&stats->lock);
+	return 0;
+}
+EXPORT_SYMBOL(ovs_accel_flow_stats_update);
+
+/* Find netdev using vport number */
+struct net_device *ovs_accel_dev_find(void *dp_inst, int vport_no)
+{
+	struct datapath *dp = dp_inst;
+	struct net_device *dev;
+	struct vport *vport;
+
+	rcu_read_lock();
+
+	vport = ovs_vport_rcu(dp, vport_no);
+	if (!vport) {
+		rcu_read_unlock();
+		return NULL;
+	}
+
+	dev = vport->dev;
+	rcu_read_unlock();
+	return dev;
+}
+EXPORT_SYMBOL(ovs_accel_dev_find);
+
+/* Find egress interface using key and skb */
+struct net_device *ovs_accel_egress_dev_find(void *dp_inst,
+					     struct sw_flow_key *key,
+					     struct sk_buff *skb)
+{
+	struct datapath *dp = dp_inst;
+	struct sw_flow *flow;
+	struct sw_flow_actions *sf_acts;
+	struct net_device *dev;
+	const struct nlattr *a;
+	int rem;
+	int egress_cnt = 0;
+
+	rcu_read_lock();
+	flow = ovs_accel_flow_find(dp_inst, key);
+	if (unlikely(!flow))
+		goto done;
+
+	sf_acts = rcu_dereference(flow->sf_acts);
+	for (a = sf_acts->actions, rem = sf_acts->actions_len; rem > 0;
+			     a = nla_next(a, &rem)) {
+		struct vport *vport;
+		int port_no;
+
+		switch (nla_type(a)) {
+		case OVS_ACTION_ATTR_OUTPUT:
+			port_no = nla_get_u32(a);
+			vport = ovs_vport_ovsl_rcu(dp, port_no);
+
+			/*
+			 * Avoid offloading flows with internal port as egress port
+			 */
+			if (!vport || (vport->ops->type == OVS_VPORT_TYPE_INTERNAL)) {
+				goto done;
+			}
+
+			dev = vport->dev;
+			egress_cnt++;
+		}
+	}
+
+	/*
+	 * Return dev only if flow is not a broadcast i.e.
+	 * there is only one egress interface for the flow
+	 */
+	if (egress_cnt == 1) {
+		dev_hold(dev);
+		rcu_read_unlock();
+		return dev;
+	}
+
+done:
+	rcu_read_unlock();
+	return NULL;
+}
+EXPORT_SYMBOL(ovs_accel_egress_dev_find);
+
 static int ovs_vport_cmd_get(struct sk_buff *skb, struct genl_info *info)
 {
 	struct nlattr **a = info->attrs;
@@ -2661,6 +3061,8 @@ static int __init dp_init(void)
 
 	pr_info("Open vSwitch switching datapath\n");
 
+	init_srcu_struct(&ovs_accel_cb_sp_rcu);
+
 	err = action_fifos_init();
 	if (err)
 		goto error;
@@ -2710,6 +3112,7 @@ error_unreg_rtnl_link:
 error_action_fifos_exit:
 	action_fifos_exit();
 error:
+	cleanup_srcu_struct(&ovs_accel_cb_sp_rcu);
 	return err;
 }
 
--- a/net/openvswitch/datapath.h
+++ b/net/openvswitch/datapath.h
@@ -174,6 +174,37 @@ enum ovs_pkt_hash_types {
 	OVS_PACKET_HASH_L4_BIT = (1ULL << 33),
 };
 
+/**
+ *	struct ovs_accel_callback - OVS acceleration callbacks
+ *	@ovs_accel_dp_add - new data path is created
+ *	@ovs_accel_dp_del - data path is deleted
+ *	@ovs_accel_dp_port_add - new port is added into data path
+ *	@ovs_accel_dp_port_del - port is deleted from data path
+ *	@ovs_accel_dp_flow_add - new flow rule is added in data path
+ *	@ovs_accel_dp_flow_del - flow rule is deleted from data path
+ *	@ovs_accel_dp_flow_set - existing flow rule is modified in data path
+ *	@ovs_accel_dp_flow_tbl_flush - flow table is flushed in data path
+ *	@ovs_accel_dp_pkt_process - Process data path packet
+ */
+struct ovs_accel_callback {
+	void (*ovs_accel_dp_add)(void *dp, struct net_device *dev);
+	void (*ovs_accel_dp_del)(void *dp, struct net_device *dev);
+	void (*ovs_accel_dp_port_add)(void *dp, void *vp,
+				      int vp_num, enum ovs_vport_type vp_type,
+				      const char *master, struct net_device *dev);
+	void (*ovs_accel_dp_port_del)(void *dp,  void *vp,
+				      struct net_device *dev);
+	void (*ovs_accel_dp_flow_add)(void *dp, struct sw_flow *sf);
+	void (*ovs_accel_dp_flow_del)(void *dp, struct sw_flow *sf);
+	void (*ovs_accel_dp_flow_set)(void *dp, struct sw_flow *sf,
+				      struct sw_flow_actions *sfa);
+	void (*ovs_accel_dp_flow_tbl_flush)(void *dp);
+	void (*ovs_accel_dp_pkt_process)(void *dp, struct sk_buff *skb,
+					 struct sw_flow_key *key,
+					 struct sw_flow *sf,
+					 struct sw_flow_actions *sfa);
+};
+
 extern unsigned int ovs_net_id;
 void ovs_lock(void);
 void ovs_unlock(void);
@@ -260,6 +291,7 @@ void ovs_dp_detach_port(struct vport *);
 int ovs_dp_upcall(struct datapath *, struct sk_buff *,
 		  const struct sw_flow_key *, const struct dp_upcall_info *,
 		  uint32_t cutlen);
+void ovs_dp_port_del_notify(struct datapath *dp, struct vport *vp);
 
 u32 ovs_dp_get_upcall_portid(const struct datapath *dp, uint32_t cpu_id);
 
@@ -275,6 +307,21 @@ void ovs_dp_notify_wq(struct work_struct
 int action_fifos_init(void);
 void action_fifos_exit(void);
 
+int ovs_register_accelerator(struct ovs_accel_callback *oac);
+void ovs_unregister_accelerator(struct ovs_accel_callback *oac);
+int ovs_accel_flow_stats_update(void *dp, void *out_vport,
+				struct sw_flow_key *sf, int pkts, int bytes);
+struct sw_flow *ovs_accel_flow_find(void *dp, struct sw_flow_key *sfk);
+struct net_device *ovs_accel_dev_find(void *dp, int vport_no);
+struct net_device *ovs_accel_egress_dev_find(void *dp_inst,
+					     struct sw_flow_key *key,
+					     struct sk_buff *skb);
+
+struct sw_flow *ovs_accel_flow_find_by_mac(void *dp_inst,
+						struct net_device *indev,
+						struct net_device *outdev,
+						uint8_t *smac, uint8_t *dmac, uint16_t type);
+
 /* 'KEY' must not have any bits set outside of the 'MASK' */
 #define OVS_MASKED(OLD, KEY, MASK) ((KEY) | ((OLD) & ~(MASK)))
 #define OVS_SET_MASKED(OLD, KEY, MASK) ((OLD) = OVS_MASKED(OLD, KEY, MASK))
--- a/net/openvswitch/dp_notify.c
+++ b/net/openvswitch/dp_notify.c
@@ -19,6 +19,7 @@ static void dp_detach_port_notify(struct
 	dp = vport->dp;
 	notify = ovs_vport_cmd_build_info(vport, ovs_dp_get_net(dp),
 					  0, 0, OVS_VPORT_CMD_DEL);
+	ovs_dp_port_del_notify(vport->dp, vport);
 	ovs_dp_detach_port(vport);
 	if (IS_ERR(notify)) {
 		genl_set_err(&dp_vport_genl_family, ovs_dp_get_net(dp), 0,
